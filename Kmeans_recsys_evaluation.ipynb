{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Metric 2] Cluster Separation & Decision Boundary Confidence\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def calculate_separation_metrics(X, labels, centroids):\n",
    "    \"\"\"\n",
    "    For each point, calculate:\n",
    "    1. Distance to assigned cluster centroid (d1)\n",
    "    2. Distance to nearest other cluster centroid (d2)\n",
    "    3. Separation ratio: (d2 - d1) / d1 (higher = more confident assignment)\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'cluster': [],\n",
    "        'assigned_distance': [],\n",
    "        'nearest_other_distance': [],\n",
    "        'separation_ratio': [],\n",
    "        'nearest_other_cluster': []\n",
    "    }\n",
    "    \n",
    "    for i, point in enumerate(X):\n",
    "        assigned_cluster = labels[i]\n",
    "        \n",
    "        # Distance to assigned centroid\n",
    "        d1 = np.linalg.norm(point - centroids[assigned_cluster])\n",
    "        \n",
    "        # Find nearest other cluster\n",
    "        other_distances = {}\n",
    "        for cid, centroid in centroids.items():\n",
    "            if cid != assigned_cluster:\n",
    "                other_distances[cid] = np.linalg.norm(point - centroid)\n",
    "        \n",
    "        nearest_other_cluster = min(other_distances, key=other_distances.get)\n",
    "        d2 = other_distances[nearest_other_cluster]\n",
    "        \n",
    "        # Separation ratio: higher means more confident assignment\n",
    "        sep_ratio = (d2 - d1) / d1 if d1 > 0 else np.inf\n",
    "        \n",
    "        results['cluster'].append(assigned_cluster)\n",
    "        results['assigned_distance'].append(d1)\n",
    "        results['nearest_other_distance'].append(d2)\n",
    "        results['separation_ratio'].append(sep_ratio)\n",
    "        results['nearest_other_cluster'].append(nearest_other_cluster)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Calculate for train\n",
    "sep_train = calculate_separation_metrics(X_train, train_final, cent_train)\n",
    "\n",
    "# Calculate for test\n",
    "sep_test = calculate_separation_metrics(X_test, test_final, cent_train)\n",
    "\n",
    "# Summary by cluster\n",
    "print(\"\\nTrain Set - Cluster Assignment Confidence:\")\n",
    "print(f\"{'Cluster':<8} {'Avg Sep Ratio':<18} {'% Ambiguous (<0.3)':<20} {'Confused With':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cid in sorted(sep_train['cluster'].unique()):\n",
    "    cluster_data = sep_train[sep_train['cluster'] == cid]\n",
    "    avg_sep = cluster_data['separation_ratio'].mean()\n",
    "    pct_ambiguous = (cluster_data['separation_ratio'] < 0.3).mean() * 100\n",
    "    \n",
    "    # Most common confused cluster\n",
    "    confused_with = cluster_data['nearest_other_cluster'].mode()[0] if len(cluster_data) > 0 else 'N/A'\n",
    "    \n",
    "    status = \"✓ Clear\" if pct_ambiguous < 10 else (\"⚠ Moderate\" if pct_ambiguous < 25 else \"❌ Fuzzy\")\n",
    "    print(f\"{cid:<8} {avg_sep:<18.3f} {pct_ambiguous:>6.1f}%  {status:<12} Cluster {confused_with}\")\n",
    "\n",
    "print(\"\\nTest Set - Cluster Assignment Confidence:\")\n",
    "print(f\"{'Cluster':<8} {'Avg Sep Ratio':<18} {'% Ambiguous (<0.3)':<20} {'Confused With':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cid in sorted(sep_test['cluster'].unique()):\n",
    "    cluster_data = sep_test[sep_test['cluster'] == cid]\n",
    "    avg_sep = cluster_data['separation_ratio'].mean()\n",
    "    pct_ambiguous = (cluster_data['separation_ratio'] < 0.3).mean() * 100\n",
    "    \n",
    "    confused_with = cluster_data['nearest_other_cluster'].mode()[0] if len(cluster_data) > 0 else 'N/A'\n",
    "    \n",
    "    status = \"✓ Clear\" if pct_ambiguous < 10 else (\"⚠ Moderate\" if pct_ambiguous < 25 else \"❌ Fuzzy\")\n",
    "    print(f\"{cid:<8} {avg_sep:<18.3f} {pct_ambiguous:>6.1f}%  {status:<12} Cluster {confused_with}\")\n",
    "\n",
    "# Identify most ambiguous assignments in test set\n",
    "print(\"\\n⚠ Most Ambiguous Assignments in Test Set (Separation Ratio < 0.2):\")\n",
    "ambiguous = sep_test[sep_test['separation_ratio'] < 0.2].copy()\n",
    "ambiguous = ambiguous.sort_values('separation_ratio')\n",
    "print(f\"Found {len(ambiguous)} ambiguous assignments ({len(ambiguous)/len(sep_test)*100:.1f}% of test set)\")\n",
    "print(f\"\\n{'Index':<8} {'Assigned':<10} {'Could be':<10} {'Sep Ratio':<12}\")\n",
    "print(\"-\"*50)\n",
    "for idx, row in ambiguous.head(10).iterrows():\n",
    "    print(f\"{idx:<8} {int(row['cluster']):<10} {int(row['nearest_other_cluster']):<10} {row['separation_ratio']:<12.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Metric 3] Feature Contribution to Cluster Assignment\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def calculate_feature_importance_for_assignment(X, labels, centroids, feature_names):\n",
    "    \"\"\"\n",
    "    For each cluster, calculate which features most distinguish it from others.\n",
    "    Uses the ratio of within-cluster variance to between-cluster variance per feature.\n",
    "    \"\"\"\n",
    "    feature_importance = {}\n",
    "    \n",
    "    for cid in np.unique(labels):\n",
    "        cluster_points = X[labels == cid]\n",
    "        other_points = X[labels != cid]\n",
    "        \n",
    "        if len(cluster_points) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For each feature, calculate separation\n",
    "        importance_scores = []\n",
    "        for f in range(X.shape[1]):\n",
    "            cluster_vals = cluster_points[:, f]\n",
    "            other_vals = other_points[:, f]\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            mean_diff = abs(cluster_vals.mean() - other_vals.mean())\n",
    "            pooled_std = np.sqrt((cluster_vals.std()**2 + other_vals.std()**2) / 2)\n",
    "            \n",
    "            if pooled_std > 0:\n",
    "                cohens_d = mean_diff / pooled_std\n",
    "            else:\n",
    "                cohens_d = 0\n",
    "            \n",
    "            importance_scores.append(cohens_d)\n",
    "        \n",
    "        feature_importance[cid] = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance_scores\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Get feature names (original features before UMAP)\n",
    "# You'll need to use your original feature columns here\n",
    "# For now, if you want to do this on UMAP dimensions:\n",
    "umap_feature_names = [f\"UMAP_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "feat_importance_train = calculate_feature_importance_for_assignment(\n",
    "    X_train, train_final, cent_train, umap_feature_names\n",
    ")\n",
    "\n",
    "print(\"\\nTop 3 Distinguishing Features per Cluster (Train Set):\")\n",
    "print(\"-\"*70)\n",
    "for cid in sorted(feat_importance_train.keys()):\n",
    "    print(f\"\\nCluster {cid}:\")\n",
    "    top_features = feat_importance_train[cid].head(3)\n",
    "    for _, row in top_features.iterrows():\n",
    "        print(f\"  {row['Feature']:<15} Importance: {row['Importance']:.3f}\")\n",
    "\n",
    "# For TEST set - check if feature importance pattern holds\n",
    "feat_importance_test = calculate_feature_importance_for_assignment(\n",
    "    X_test, test_final, cent_train, umap_feature_names\n",
    ")\n",
    "\n",
    "print(\"\\n\\nFeature Importance Consistency (Train vs Test):\")\n",
    "print(f\"{'Cluster':<8} {'Train Top Feat':<20} {'Test Top Feat':<20} {'Match?':<10}\")\n",
    "print(\"-\"*70)\n",
    "for cid in sorted(feat_importance_train.keys()):\n",
    "    if cid in feat_importance_test:\n",
    "        train_top = feat_importance_train[cid].iloc[0]['Feature']\n",
    "        test_top = feat_importance_test[cid].iloc[0]['Feature']\n",
    "        match = \"✓\" if train_top == test_top else \"✗\"\n",
    "        print(f\"{cid:<8} {train_top:<20} {test_top:<20} {match:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Metric 4] Cluster Membership Stability\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def calculate_stability_score(X, labels, centroids, n_bootstrap=50):\n",
    "    \"\"\"\n",
    "    For each point, see how often it gets assigned to the same cluster\n",
    "    when we add noise or bootstrap sample the data.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    stability_scores = np.zeros(n_samples)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Add small Gaussian noise\n",
    "        noise_std = 0.05 * X.std(axis=0)  # 5% noise\n",
    "        X_noisy = X + np.random.normal(0, noise_std, X.shape)\n",
    "        \n",
    "        # Reassign to nearest centroid\n",
    "        new_labels = np.array([\n",
    "            min(centroids.keys(), \n",
    "                key=lambda c: np.linalg.norm(X_noisy[i] - centroids[c]))\n",
    "            for i in range(n_samples)\n",
    "        ])\n",
    "        \n",
    "        # Check if assignment matches original\n",
    "        stability_scores += (new_labels == labels).astype(int)\n",
    "    \n",
    "    return stability_scores / n_bootstrap\n",
    "\n",
    "# Calculate stability for test set\n",
    "stability_test = calculate_stability_score(X_test, test_final, cent_train, n_bootstrap=50)\n",
    "\n",
    "# Summary by cluster\n",
    "stability_df = pd.DataFrame({\n",
    "    'cluster': test_final,\n",
    "    'stability': stability_test\n",
    "})\n",
    "\n",
    "print(\"\\nCluster Assignment Stability (Test Set):\")\n",
    "print(f\"{'Cluster':<8} {'Mean Stability':<18} {'% Unstable (<0.7)':<20} {'Status':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cid in sorted(stability_df['cluster'].unique()):\n",
    "    cluster_stab = stability_df[stability_df['cluster'] == cid]['stability']\n",
    "    mean_stab = cluster_stab.mean()\n",
    "    pct_unstable = (cluster_stab < 0.7).mean() * 100\n",
    "    \n",
    "    status = \"✓ Robust\" if pct_unstable < 10 else (\"⚠ Moderate\" if pct_unstable < 25 else \"❌ Fragile\")\n",
    "    print(f\"{cid:<8} {mean_stab:<18.3f} {pct_unstable:>6.1f}%  {status:<15} {status}\")\n",
    "\n",
    "print(f\"\\nOverall Test Set Stability: {stability_test.mean():.3f}\")\n",
    "print(f\"Samples with stability < 0.7: {(stability_test < 0.7).sum()} ({(stability_test < 0.7).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06691bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1db48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add8cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c337be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
