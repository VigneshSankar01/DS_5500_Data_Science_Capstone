{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d6f145b-fd39-4698-b4f6-5df89d224e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "753bbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def preprocess_new_record(new_record, pipeline_path='Data/preprocessing_pipeline.pkl'):\n",
    "    \"\"\"\n",
    "    Apply the same preprocessing to new records\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    new_record : dict or pd.DataFrame\n",
    "        New record(s) to preprocess\n",
    "    pipeline_path : str\n",
    "        Path to saved preprocessing pipeline\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Preprocessed record matching training data format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pipeline\n",
    "    with open(pipeline_path, 'rb') as f:\n",
    "        pipeline = pickle.load(f)\n",
    "    \n",
    "    # Convert to DataFrame if needed\n",
    "    if isinstance(new_record, dict):\n",
    "        df_new = pd.DataFrame([new_record])\n",
    "    else:\n",
    "        df_new = new_record.copy()\n",
    "    \n",
    "    # Step 1: Normalize Unknown-like responses\n",
    "    df_new = df_new.replace({\n",
    "        \"Don't know\": \"Unknown\", \"Refused\": \"Unknown\", \n",
    "        \"Not Applicable\": \"Unknown\", \"N/A\": \"Unknown\", \n",
    "        \"Unknown/NA\": \"Unknown\"\n",
    "    })\n",
    "    \n",
    "    # Step 2: Binary encoding\n",
    "    for col in pipeline['binary_cols']:\n",
    "        if col not in df_new.columns:\n",
    "            continue\n",
    "        \n",
    "        if col == \"Has_diabetes\":\n",
    "            mapping = pipeline['binary_mappings'][\"Has_diabetes\"]\n",
    "        elif col == \"Received_Hepatitis_A_Vaccine\":\n",
    "            mapping = pipeline['binary_mappings'][\"Received_Hepatitis_A_Vaccine\"]\n",
    "        else:\n",
    "            mapping = pipeline['binary_mappings'][\"default\"]\n",
    "        \n",
    "        df_new[col] = df_new[col].map(mapping)\n",
    "    \n",
    "    # Step 3: Ordinal encoding\n",
    "    for col, encoder in pipeline['ordinal_encoders'].items():\n",
    "        if col in df_new.columns:\n",
    "            df_new[col] = encoder.transform(df_new[[col]])\n",
    "    \n",
    "    # Step 4: One-hot encoding\n",
    "    for base_col in pipeline['ohe_cols']:\n",
    "        if base_col in df_new.columns:\n",
    "            # Get dummies for this column\n",
    "            dummies = pd.get_dummies(df_new[base_col], prefix=base_col, dtype=int)\n",
    "            \n",
    "            # Add any missing columns from training\n",
    "            for train_col in pipeline['ohe_column_names']:\n",
    "                if base_col in train_col and train_col not in dummies.columns:\n",
    "                    dummies[train_col] = 0\n",
    "            \n",
    "            # Remove extra columns not in training\n",
    "            cols_to_keep = [col for col in dummies.columns \n",
    "                           if col in pipeline['ohe_column_names']]\n",
    "            dummies = dummies[cols_to_keep]\n",
    "            \n",
    "            # Add to dataframe\n",
    "            df_new = pd.concat([df_new.drop(columns=[base_col]), dummies], axis=1)\n",
    "    \n",
    "    # Step 5: Apply log transformation to skewed columns\n",
    "    for col in pipeline['skewed_cols']:\n",
    "        if col in df_new.columns:\n",
    "            df_new[col] = np.log1p(df_new[col].clip(lower=0))\n",
    "    \n",
    "    # Step 6: Ensure all columns from training exist\n",
    "    for col in pipeline['all_columns']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 0  # Add missing columns with default value\n",
    "    \n",
    "    # Step 7: Reorder columns to match training data\n",
    "    df_new = df_new[pipeline['all_columns']]\n",
    "    \n",
    "    # Step 8: Apply scaling\n",
    "    df_new[pipeline['cols_to_scale']] = pipeline['scaler'].transform(\n",
    "        df_new[pipeline['cols_to_scale']]\n",
    "    )\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2c1d87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of processed record: (1050, 55)\n",
      "Matches training data shape: True\n"
     ]
    }
   ],
   "source": [
    "# Example new patient record\n",
    "test_df = pd.read_csv('Data/test_dataset.csv')\n",
    "df_scaled = pd.read_csv('Data/df_scaled.csv')\n",
    "df_scaled.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "# Preprocess the new record\n",
    "processed_patient = preprocess_new_record(test_df)\n",
    "\n",
    "print(f\"Shape of processed record: {processed_patient.shape}\")\n",
    "print(f\"Matches training data shape: {processed_patient.shape[1] == df_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee0418",
   "metadata": {},
   "source": [
    "# Dim Reduction UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60e68930-e6bd-4abc-ad7d-4d0cda42542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f78dd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 22 16:16:14 2025 Building and compiling search function\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9f75d3f9b24f548ff862d04b527eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/100 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  100 epochs\n",
      "\tcompleted  10  /  100 epochs\n",
      "\tcompleted  20  /  100 epochs\n",
      "\tcompleted  30  /  100 epochs\n",
      "\tcompleted  40  /  100 epochs\n",
      "\tcompleted  50  /  100 epochs\n",
      "\tcompleted  60  /  100 epochs\n",
      "\tcompleted  70  /  100 epochs\n",
      "\tcompleted  80  /  100 epochs\n",
      "\tcompleted  90  /  100 epochs\n",
      "(1050, 20)\n",
      "       UMAP1      UMAP2     UMAP3     UMAP4     UMAP5     UMAP6     UMAP7  \\\n",
      "0  13.024805  13.073879  5.157461  2.991746  5.764147  5.317441  6.363474   \n",
      "1  -4.150143   5.798073  5.107178  6.727241  4.209681  2.940264 -1.061531   \n",
      "2  12.650412  12.129565  5.002039  3.329209  5.700506  6.181603  5.701279   \n",
      "3  12.410887   9.398935  4.929136  3.380293  5.901017  5.174147  5.899176   \n",
      "4  -4.033573   5.764928  5.089195  6.560227  4.229160  2.989349 -0.971349   \n",
      "\n",
      "      UMAP8     UMAP9    UMAP10    UMAP11    UMAP12    UMAP13    UMAP14  \\\n",
      "0  6.094333  1.970684  3.853272  6.677246  4.087493  3.857002  2.361790   \n",
      "1  5.780765  7.538804  4.759531  6.841758  3.087821  8.424713  8.717398   \n",
      "2  7.108046  4.273507  3.404667  7.138408  4.130431  3.944384  3.475381   \n",
      "3  5.939536  2.063258  3.597143  6.700058  4.690133  3.555218  3.359969   \n",
      "4  5.759371  7.471111  4.778776  6.771988  2.911294  8.289840  8.615449   \n",
      "\n",
      "     UMAP15    UMAP16    UMAP17    UMAP18    UMAP19    UMAP20  \n",
      "0  2.919640  2.792840  6.533456  4.453222  0.049239  1.572855  \n",
      "1  5.998413  4.511382  3.324929  3.264351  4.680604  5.339237  \n",
      "2  3.410046  3.692559  5.797626  4.835593  0.250794  1.935026  \n",
      "3  2.902275  3.136115  6.473524  5.662247  0.888806  2.056911  \n",
      "4  6.082884  4.492987  3.323495  3.305792  4.667191  5.574940  \n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "umap = joblib.load('Data/umap_model.pkl')\n",
    "umap_test = umap.transform(processed_patient)\n",
    "umap_test_df = pd.DataFrame(umap_test, columns=[f'UMAP{i+1}' for i in range(umap_test.shape[1])])\n",
    "umap_test_df.head()\n",
    "\n",
    "print(umap_test_df.shape)\n",
    "print(umap_test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5b1d0",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50a75596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING CLUSTERING MODELS\n",
      "======================================================================\n",
      "✓ All models loaded successfully\n",
      "\n",
      "======================================================================\n",
      "ASSIGNING CLUSTERS TO TEST PATIENTS\n",
      "======================================================================\n",
      "Test data shape: (1050, 20)\n",
      "\n",
      "======================================================================\n",
      "UMAP CLUSTER ASSIGNMENT SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Initial cluster distribution (4 clusters):\n",
      "Initial_Cluster_UMAP\n",
      "0    574\n",
      "1    173\n",
      "2    132\n",
      "3    171\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final cluster distribution (7 clusters):\n",
      "Final_Cluster_UMAP\n",
      "0    457\n",
      "1      2\n",
      "2    115\n",
      "3    173\n",
      "4    132\n",
      "5     46\n",
      "6    125\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cluster mapping (UMAP-based):\n",
      "  Final Cluster 0: Initial Cluster 0 → Subcluster 0\n",
      "  Final Cluster 1: Initial Cluster 0 → Subcluster 1\n",
      "  Final Cluster 2: Initial Cluster 0 → Subcluster 2\n",
      "  Final Cluster 3: Initial Cluster 1 (no subclustering)\n",
      "  Final Cluster 4: Initial Cluster 2 (no subclustering)\n",
      "  Final Cluster 5: Initial Cluster 3 → Subcluster 0\n",
      "  Final Cluster 6: Initial Cluster 3 → Subcluster 1\n",
      "\n",
      "First 20 patient assignments:\n",
      "    Initial_Cluster_UMAP  Subcluster_UMAP  Final_Cluster_UMAP\n",
      "0                      0              0.0                   0\n",
      "1                      1              NaN                   3\n",
      "2                      3              1.0                   6\n",
      "3                      2              NaN                   4\n",
      "4                      1              NaN                   3\n",
      "5                      0              0.0                   0\n",
      "6                      2              NaN                   4\n",
      "7                      0              0.0                   0\n",
      "8                      1              NaN                   3\n",
      "9                      2              NaN                   4\n",
      "10                     0              0.0                   0\n",
      "11                     0              0.0                   0\n",
      "12                     0              2.0                   2\n",
      "13                     0              0.0                   0\n",
      "14                     1              NaN                   3\n",
      "15                     0              0.0                   0\n",
      "16                     3              1.0                   6\n",
      "17                     3              0.0                   5\n",
      "18                     0              2.0                   2\n",
      "19                     2              NaN                   4\n",
      "\n",
      "✓ Results saved to 'Data/test_patients_with_clusters_kmeans_umap.csv'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# LOAD CLUSTERING MODELS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING CLUSTERING MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "kmeans_final = joblib.load('Models/kmeans_umap_initial4_model.pkl')\n",
    "kmeans_c0    = joblib.load('Models/kmeans_umap_c0.pkl')  \n",
    "kmeans_c3    = joblib.load('Models/kmeans_umap_c3.pkl')\n",
    "\n",
    "k_split_c0 = 3\n",
    "k_split_c3 = 2\n",
    "\n",
    "print(\"✓ All models loaded successfully\")\n",
    "\n",
    "# ============================================================\n",
    "# ASSIGN CLUSTERS TO ALL TEST PATIENTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ASSIGNING CLUSTERS TO TEST PATIENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Test data shape: {umap_test_df.shape}\")\n",
    "\n",
    "# Make sure columns match what KMeans was trained on (optional but safer)\n",
    "if hasattr(kmeans_final, \"feature_names_in_\"):\n",
    "    expected_cols = list(kmeans_final.feature_names_in_)\n",
    "    umap_test_df = umap_test_df[expected_cols]\n",
    "\n",
    "# <<< NEW: cast to the same dtype as the model's cluster centers\n",
    "dtype = kmeans_final.cluster_centers_.dtype\n",
    "X_test = umap_test_df.to_numpy().astype(dtype)\n",
    "\n",
    "# Step 1: Get initial cluster assignments (4 clusters)\n",
    "initial_clusters = kmeans_final.predict(X_test)\n",
    "\n",
    "# Step 2: Apply hierarchical logic to get FINAL clusters (0..6)\n",
    "final_clusters = []\n",
    "subcluster_assignments = []\n",
    "\n",
    "for idx in range(X_test.shape[0]):\n",
    "    main_cluster = initial_clusters[idx]\n",
    "    \n",
    "    # use the numpy slice so dtype stays consistent\n",
    "    patient_features = X_test[idx:idx+1, :]   # shape (1, n_features)\n",
    "    \n",
    "    if main_cluster == 0:\n",
    "        # Initial Cluster 0 splits into subclusters → Final clusters 0, 1, 2\n",
    "        subcluster_id = kmeans_c0.predict(patient_features)[0]  # {0,1,2}\n",
    "        final_cluster = 0 + subcluster_id                      # 0,1,2\n",
    "        final_clusters.append(int(final_cluster))\n",
    "        subcluster_assignments.append(int(subcluster_id))\n",
    "        \n",
    "    elif main_cluster == 1:\n",
    "        # Initial Cluster 1 stays as final cluster 3\n",
    "        final_clusters.append(3)\n",
    "        subcluster_assignments.append(None)\n",
    "        \n",
    "    elif main_cluster == 2:\n",
    "        # Initial Cluster 2 stays as final cluster 4\n",
    "        final_clusters.append(4)\n",
    "        subcluster_assignments.append(None)\n",
    "        \n",
    "    elif main_cluster == 3:\n",
    "        # Initial Cluster 3 splits into subclusters → Final clusters 5, 6\n",
    "        subcluster_id = kmeans_c3.predict(patient_features)[0]  # {0,1}\n",
    "        final_cluster = 5 + subcluster_id                      # 5,6\n",
    "        final_clusters.append(int(final_cluster))\n",
    "        subcluster_assignments.append(int(subcluster_id))\n",
    "\n",
    "# Attach results back to a DataFrame (we can reuse umap_test_df)\n",
    "umap_test_df_assigned = umap_test_df.copy()\n",
    "umap_test_df_assigned['Initial_Cluster_UMAP'] = initial_clusters\n",
    "umap_test_df_assigned['Subcluster_UMAP']      = subcluster_assignments\n",
    "umap_test_df_assigned['Final_Cluster_UMAP']   = final_clusters\n",
    "\n",
    "# ============================================================\n",
    "# DISPLAY RESULTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UMAP CLUSTER ASSIGNMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nInitial cluster distribution (4 clusters):\")\n",
    "print(umap_test_df_assigned['Initial_Cluster_UMAP'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nFinal cluster distribution (7 clusters):\")\n",
    "print(umap_test_df_assigned['Final_Cluster_UMAP'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nCluster mapping (UMAP-based):\")\n",
    "print(\"  Final Cluster 0: Initial Cluster 0 → Subcluster 0\")\n",
    "print(\"  Final Cluster 1: Initial Cluster 0 → Subcluster 1\")\n",
    "print(\"  Final Cluster 2: Initial Cluster 0 → Subcluster 2\")\n",
    "print(\"  Final Cluster 3: Initial Cluster 1 (no subclustering)\")\n",
    "print(\"  Final Cluster 4: Initial Cluster 2 (no subclustering)\")\n",
    "print(\"  Final Cluster 5: Initial Cluster 3 → Subcluster 0\")\n",
    "print(\"  Final Cluster 6: Initial Cluster 3 → Subcluster 1\")\n",
    "\n",
    "print(\"\\nFirst 20 patient assignments:\")\n",
    "print(umap_test_df_assigned[['Initial_Cluster_UMAP', 'Subcluster_UMAP', 'Final_Cluster_UMAP']].head(20))\n",
    "\n",
    "# Save results\n",
    "umap_test_df_assigned.to_csv('Data/test_patients_with_clusters_kmeans_umap.csv', index=False)\n",
    "print(\"\\n✓ Results saved to 'Data/test_patients_with_clusters_kmeans_umap.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58852f1",
   "metadata": {},
   "source": [
    "# Cluster Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca8fd7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLUSTER ASSIGNMENT VALIDATION\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLUSTER ASSIGNMENT VALIDATION\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a677e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded: (9442, 20)\n",
      "Columns: ['UMAP1', 'UMAP2', 'UMAP3', 'UMAP4', 'UMAP5', 'UMAP6', 'UMAP7', 'UMAP8', 'UMAP9', 'UMAP10', 'UMAP11', 'UMAP12', 'UMAP13', 'UMAP14', 'UMAP15', 'UMAP16', 'UMAP17', 'UMAP18', 'UMAP19', 'UMAP20']\n"
     ]
    }
   ],
   "source": [
    "# Load the training PCA data\n",
    "df_umap = pd.read_csv('Data/umap_dataset.csv')\n",
    "\n",
    "print(f\"Training data loaded: {df_umap.shape}\")\n",
    "print(f\"Columns: {df_umap.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d60d288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Metric 1] Within-Cluster Variance Stability\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Within-cluster variance comparison (Initial 4 clusters):\n",
      "Cluster    Train Variance       Test Variance        % Change       \n",
      "----------------------------------------------------------------------\n",
      "0          46.2789              46.8613              +1.26%  ✓ Stable\n",
      "1          59.9360              61.5086              +2.62%  ✓ Stable\n",
      "2          31.4401              30.6412              -2.54%  ✓ Stable\n",
      "3          43.4805              37.4921              -13.77%  ✓ Stable\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Metric 1] Within-Cluster Variance Stability\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def calculate_within_cluster_variance(data, labels, centroids):\n",
    "    \"\"\"Calculate within-cluster variance for each cluster\"\"\"\n",
    "    variances = {}\n",
    "    for cluster_id in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster_id]\n",
    "        centroid = centroids[cluster_id]\n",
    "        variance = np.mean(np.sum((cluster_points - centroid)**2, axis=1))\n",
    "        variances[cluster_id] = variance\n",
    "    return variances\n",
    "\n",
    "# For initial 4 clusters - calculate on TRAINING data\n",
    "train_initial_labels = kmeans_final.labels_  # Training labels\n",
    "train_centroids = kmeans_final.cluster_centers_\n",
    "\n",
    "# Calculate training variance\n",
    "train_variances = calculate_within_cluster_variance(\n",
    "    df_umap.drop(columns=['Cluster', 'Cluster_Original', 'Cluster_Refined'], errors='ignore').values,\n",
    "    train_initial_labels,\n",
    "    train_centroids\n",
    ")\n",
    "\n",
    "# Calculate test variance\n",
    "test_initial_labels = initial_clusters\n",
    "# ✅ Use correct object (umap_test_df if DataFrame, else pca_test if array)\n",
    "test_data = umap_test_df.values if 'umap_test_df' in locals() else umap_test\n",
    "\n",
    "test_variances = calculate_within_cluster_variance(\n",
    "    test_data,\n",
    "    test_initial_labels,\n",
    "    train_centroids\n",
    ")\n",
    "\n",
    "print(\"\\nWithin-cluster variance comparison (Initial 4 clusters):\")\n",
    "print(f\"{'Cluster':<10} {'Train Variance':<20} {'Test Variance':<20} {'% Change':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cluster_id in sorted(train_variances.keys()):\n",
    "    train_var = train_variances[cluster_id]\n",
    "    test_var = test_variances.get(cluster_id, np.nan)\n",
    "    \n",
    "    if not np.isnan(test_var):\n",
    "        pct_change = ((test_var - train_var) / train_var) * 100\n",
    "        status = \"✓ Stable\" if abs(pct_change) < 20 else \"⚠ Check\"\n",
    "        print(f\"{cluster_id:<10} {train_var:<20.4f} {test_var:<20.4f} {pct_change:>+.2f}%  {status}\")\n",
    "    else:\n",
    "        print(f\"{cluster_id:<10} {train_var:<20.4f} {'No test samples':<20} {'N/A':<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ca1fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Within-cluster variance comparison (Final UMAP clusters):\n",
      "Cluster  Train Var          Test Var           % Change    \n",
      "------------------------------------------------------------\n",
      "0        1.4662             1.8775               +28.05%  ⚠ Check\n",
      "1        8.3668             0.0002              -100.00%  ⚠ Check\n",
      "2        1.0266             0.9766                -4.88%  ✓ Stable\n",
      "3        1.6836             2.0313               +20.65%  ✓ Stable\n",
      "4        0.9731             0.7724               -20.63%  ✓ Stable\n",
      "5        0.9464             0.4574               -51.66%  ⚠ Check\n",
      "6        0.7392             0.7756                +4.92%  ✓ Stable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_centroids_and_variances(X, labels):\n",
    "    \"\"\"Return centroids and within-cluster variances for given labels.\"\"\"\n",
    "    centroids = {}\n",
    "    variances = {}\n",
    "    for cid in np.unique(labels):\n",
    "        mask = (labels == cid)\n",
    "        pts = X[mask]\n",
    "        if pts.size == 0:\n",
    "            continue\n",
    "        c = pts.mean(axis=0)\n",
    "        centroids[cid] = c\n",
    "        d2 = np.sum((pts - c)**2, axis=1)\n",
    "        variances[cid] = d2.mean()\n",
    "    return centroids, variances\n",
    "\n",
    "# ============================================================\n",
    "# BUILD TRAIN UMAP MATRIX + FINAL CLUSTER LABELS\n",
    "# ============================================================\n",
    "umap_cols = [c for c in df_umap.columns if c.startswith(\"UMAP\")]\n",
    "train_umap_df = df_umap[umap_cols]\n",
    "\n",
    "# Ensure same column order as used to train kmeans_final\n",
    "if hasattr(kmeans_final, \"feature_names_in_\"):\n",
    "    expected_cols = list(kmeans_final.feature_names_in_)\n",
    "    train_umap_df = train_umap_df[expected_cols]\n",
    "\n",
    "# Match dtype to model\n",
    "X_train = train_umap_df.to_numpy().astype(kmeans_final.cluster_centers_.dtype)\n",
    "train_initial = kmeans_final.labels_  # 0..3 on train\n",
    "\n",
    "train_final = np.empty_like(train_initial)\n",
    "\n",
    "for i, main in enumerate(train_initial):\n",
    "    point = X_train[i:i+1, :]  # shape (1, d)\n",
    "    \n",
    "    if main == 0:\n",
    "        # initial 0 → subclusters 0,1,2 → final 0,1,2\n",
    "        sub = kmeans_c0.predict(point)[0]      # 0,1,2\n",
    "        final = 0 + sub\n",
    "    elif main == 1:\n",
    "        # initial 1 → final 3\n",
    "        final = 3\n",
    "    elif main == 2:\n",
    "        # initial 2 → final 4\n",
    "        final = 4\n",
    "    elif main == 3:\n",
    "        # initial 3 → subclusters 0,1 → final 5,6\n",
    "        sub = kmeans_c3.predict(point)[0]      # 0,1\n",
    "        final = 5 + sub\n",
    "    \n",
    "    train_final[i] = int(final)\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN VARIANCES (FINAL CLUSTERS)\n",
    "# ============================================================\n",
    "cent_train, var_train = compute_centroids_and_variances(X_train, train_final)\n",
    "\n",
    "# ============================================================\n",
    "# TEST VARIANCES (FINAL CLUSTERS)\n",
    "# ============================================================\n",
    "test_umap_df = umap_test_df_assigned[umap_cols]\n",
    "if hasattr(kmeans_final, \"feature_names_in_\"):\n",
    "    test_umap_df = test_umap_df[expected_cols]\n",
    "\n",
    "X_test = test_umap_df.to_numpy().astype(kmeans_final.cluster_centers_.dtype)\n",
    "test_final = umap_test_df_assigned[\"Final_Cluster_UMAP\"].to_numpy()\n",
    "\n",
    "_, var_test = compute_centroids_and_variances(X_test, test_final)\n",
    "\n",
    "# ============================================================\n",
    "# COMPARISON: TRAIN vs TEST WITH % CHANGE\n",
    "# ============================================================\n",
    "print(\"\\nWithin-cluster variance comparison (Final UMAP clusters):\")\n",
    "print(f\"{'Cluster':<8} {'Train Var':<18} {'Test Var':<18} {'% Change':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for cid in sorted(var_train.keys()):\n",
    "    tv = var_train[cid]\n",
    "    sv = var_test.get(cid, np.nan)\n",
    "    \n",
    "    if np.isnan(sv):\n",
    "        print(f\"{cid:<8} {tv:<18.4f} {'No test pts':<18} {'N/A':<12}\")\n",
    "    else:\n",
    "        pct = ((sv - tv) / tv * 100) if tv != 0 else np.nan\n",
    "        tag = \"✓ Stable\" if (not np.isnan(pct) and abs(pct) < 25) else \"⚠ Check\"\n",
    "        print(f\"{cid:<8} {tv:<18.4f} {sv:<18.4f} {pct:>+8.2f}%  {tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac9da4-18ef-4792-9c29-e8a2b9693953",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "423f3598-a48a-4cef-8244-425f97144e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Metric 2] Cluster Separation & Decision Boundary Confidence\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Train Set - Cluster Assignment Confidence:\n",
      "Cluster  Avg Sep Ratio      % Ambiguous (<0.3)   Confused With  \n",
      "----------------------------------------------------------------------\n",
      "0        1.035                28.9%  ❌ Fuzzy      Cluster 5\n",
      "1        0.393                32.3%  ❌ Fuzzy      Cluster 2\n",
      "2        3.588                18.8%  ⚠ Moderate   Cluster 0\n",
      "3        32.003                0.4%  ✓ Clear      Cluster 1\n",
      "4        4.124                 2.3%  ✓ Clear      Cluster 5\n",
      "5        1.356                 9.0%  ✓ Clear      Cluster 0\n",
      "6        2.191                 0.1%  ✓ Clear      Cluster 5\n",
      "\n",
      "Test Set - Cluster Assignment Confidence:\n",
      "Cluster  Avg Sep Ratio      % Ambiguous (<0.3)   Confused With  \n",
      "----------------------------------------------------------------------\n",
      "0        0.841                36.8%  ❌ Fuzzy      Cluster 5\n",
      "1        0.662                 0.0%  ✓ Clear      Cluster 2\n",
      "2        -0.459               98.3%  ❌ Fuzzy      Cluster 5\n",
      "3        30.175                0.6%  ✓ Clear      Cluster 1\n",
      "4        4.235                 0.0%  ✓ Clear      Cluster 5\n",
      "5        1.310                 8.7%  ✓ Clear      Cluster 2\n",
      "6        2.200                 0.0%  ✓ Clear      Cluster 5\n",
      "\n",
      "⚠ Most Ambiguous Assignments in Test Set (Separation Ratio < 0.2):\n",
      "Found 278 ambiguous assignments (26.5% of test set)\n",
      "\n",
      "Index    Assigned   Could be   Sep Ratio   \n",
      "--------------------------------------------------\n",
      "895      2          5          -0.822      \n",
      "762      2          5          -0.814      \n",
      "945      0          2          -0.798      \n",
      "923      2          5          -0.790      \n",
      "128      2          5          -0.778      \n",
      "824      0          2          -0.763      \n",
      "97       0          2          -0.763      \n",
      "44       0          2          -0.763      \n",
      "718      2          5          -0.750      \n",
      "689      2          5          -0.740      \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Metric 2] Cluster Separation & Decision Boundary Confidence\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def calculate_separation_metrics(X, labels, centroids):\n",
    "    \"\"\"\n",
    "    For each point, calculate:\n",
    "    1. Distance to assigned cluster centroid (d1)\n",
    "    2. Distance to nearest other cluster centroid (d2)\n",
    "    3. Separation ratio: (d2 - d1) / d1 (higher = more confident assignment)\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'cluster': [],\n",
    "        'assigned_distance': [],\n",
    "        'nearest_other_distance': [],\n",
    "        'separation_ratio': [],\n",
    "        'nearest_other_cluster': []\n",
    "    }\n",
    "    \n",
    "    for i, point in enumerate(X):\n",
    "        assigned_cluster = labels[i]\n",
    "        \n",
    "        # Distance to assigned centroid\n",
    "        d1 = np.linalg.norm(point - centroids[assigned_cluster])\n",
    "        \n",
    "        # Find nearest other cluster\n",
    "        other_distances = {}\n",
    "        for cid, centroid in centroids.items():\n",
    "            if cid != assigned_cluster:\n",
    "                other_distances[cid] = np.linalg.norm(point - centroid)\n",
    "        \n",
    "        nearest_other_cluster = min(other_distances, key=other_distances.get)\n",
    "        d2 = other_distances[nearest_other_cluster]\n",
    "        \n",
    "        # Separation ratio: higher means more confident assignment\n",
    "        sep_ratio = (d2 - d1) / d1 if d1 > 0 else np.inf\n",
    "        \n",
    "        results['cluster'].append(assigned_cluster)\n",
    "        results['assigned_distance'].append(d1)\n",
    "        results['nearest_other_distance'].append(d2)\n",
    "        results['separation_ratio'].append(sep_ratio)\n",
    "        results['nearest_other_cluster'].append(nearest_other_cluster)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Calculate for train\n",
    "sep_train = calculate_separation_metrics(X_train, train_final, cent_train)\n",
    "\n",
    "# Calculate for test\n",
    "sep_test = calculate_separation_metrics(X_test, test_final, cent_train)\n",
    "\n",
    "# Summary by cluster\n",
    "print(\"\\nTrain Set - Cluster Assignment Confidence:\")\n",
    "print(f\"{'Cluster':<8} {'Avg Sep Ratio':<18} {'% Ambiguous (<0.3)':<20} {'Confused With':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cid in sorted(sep_train['cluster'].unique()):\n",
    "    cluster_data = sep_train[sep_train['cluster'] == cid]\n",
    "    avg_sep = cluster_data['separation_ratio'].mean()\n",
    "    pct_ambiguous = (cluster_data['separation_ratio'] < 0.3).mean() * 100\n",
    "    \n",
    "    # Most common confused cluster\n",
    "    confused_with = cluster_data['nearest_other_cluster'].mode()[0] if len(cluster_data) > 0 else 'N/A'\n",
    "    \n",
    "    status = \"✓ Clear\" if pct_ambiguous < 10 else (\"⚠ Moderate\" if pct_ambiguous < 25 else \"❌ Fuzzy\")\n",
    "    print(f\"{cid:<8} {avg_sep:<18.3f} {pct_ambiguous:>6.1f}%  {status:<12} Cluster {confused_with}\")\n",
    "\n",
    "print(\"\\nTest Set - Cluster Assignment Confidence:\")\n",
    "print(f\"{'Cluster':<8} {'Avg Sep Ratio':<18} {'% Ambiguous (<0.3)':<20} {'Confused With':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cid in sorted(sep_test['cluster'].unique()):\n",
    "    cluster_data = sep_test[sep_test['cluster'] == cid]\n",
    "    avg_sep = cluster_data['separation_ratio'].mean()\n",
    "    pct_ambiguous = (cluster_data['separation_ratio'] < 0.3).mean() * 100\n",
    "    \n",
    "    confused_with = cluster_data['nearest_other_cluster'].mode()[0] if len(cluster_data) > 0 else 'N/A'\n",
    "    \n",
    "    status = \"✓ Clear\" if pct_ambiguous < 10 else (\"⚠ Moderate\" if pct_ambiguous < 25 else \"❌ Fuzzy\")\n",
    "    print(f\"{cid:<8} {avg_sep:<18.3f} {pct_ambiguous:>6.1f}%  {status:<12} Cluster {confused_with}\")\n",
    "\n",
    "# Identify most ambiguous assignments in test set\n",
    "print(\"\\n⚠ Most Ambiguous Assignments in Test Set (Separation Ratio < 0.2):\")\n",
    "ambiguous = sep_test[sep_test['separation_ratio'] < 0.2].copy()\n",
    "ambiguous = ambiguous.sort_values('separation_ratio')\n",
    "print(f\"Found {len(ambiguous)} ambiguous assignments ({len(ambiguous)/len(sep_test)*100:.1f}% of test set)\")\n",
    "print(f\"\\n{'Index':<8} {'Assigned':<10} {'Could be':<10} {'Sep Ratio':<12}\")\n",
    "print(\"-\"*50)\n",
    "for idx, row in ambiguous.head(10).iterrows():\n",
    "    print(f\"{idx:<8} {int(row['cluster']):<10} {int(row['nearest_other_cluster']):<10} {row['separation_ratio']:<12.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "591cbb4f-cb91-4294-84a8-4fac7c8df079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Metric 3] Feature Contribution to Cluster Assignment\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Top 3 Distinguishing Features per Cluster (Train Set):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Cluster 0:\n",
      "  UMAP_8          Importance: 1.524\n",
      "  UMAP_1          Importance: 1.430\n",
      "  UMAP_15         Importance: 1.391\n",
      "\n",
      "Cluster 1:\n",
      "  UMAP_11         Importance: 2.421\n",
      "  UMAP_10         Importance: 2.117\n",
      "  UMAP_9          Importance: 1.947\n",
      "\n",
      "Cluster 2:\n",
      "  UMAP_15         Importance: 1.672\n",
      "  UMAP_10         Importance: 1.653\n",
      "  UMAP_1          Importance: 1.254\n",
      "\n",
      "Cluster 3:\n",
      "  UMAP_0          Importance: 53.883\n",
      "  UMAP_12         Importance: 16.411\n",
      "  UMAP_3          Importance: 14.678\n",
      "\n",
      "Cluster 4:\n",
      "  UMAP_17         Importance: 2.511\n",
      "  UMAP_11         Importance: 2.149\n",
      "  UMAP_10         Importance: 1.012\n",
      "\n",
      "Cluster 5:\n",
      "  UMAP_7          Importance: 2.082\n",
      "  UMAP_18         Importance: 1.280\n",
      "  UMAP_9          Importance: 1.180\n",
      "\n",
      "Cluster 6:\n",
      "  UMAP_7          Importance: 2.129\n",
      "  UMAP_5          Importance: 1.687\n",
      "  UMAP_10         Importance: 1.034\n",
      "\n",
      "\n",
      "Feature Importance Consistency (Train vs Test):\n",
      "Cluster  Train Top Feat       Test Top Feat        Match?    \n",
      "----------------------------------------------------------------------\n",
      "0        UMAP_8               UMAP_8               ✓         \n",
      "1        UMAP_11              UMAP_2               ✗         \n",
      "2        UMAP_15              UMAP_7               ✗         \n",
      "3        UMAP_0               UMAP_0               ✓         \n",
      "4        UMAP_17              UMAP_17              ✓         \n",
      "5        UMAP_7               UMAP_7               ✓         \n",
      "6        UMAP_7               UMAP_7               ✓         \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Metric 3] Feature Contribution to Cluster Assignment\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def calculate_feature_importance_for_assignment(X, labels, centroids, feature_names):\n",
    "    \"\"\"\n",
    "    For each cluster, calculate which features most distinguish it from others.\n",
    "    Uses the ratio of within-cluster variance to between-cluster variance per feature.\n",
    "    \"\"\"\n",
    "    feature_importance = {}\n",
    "    \n",
    "    for cid in np.unique(labels):\n",
    "        cluster_points = X[labels == cid]\n",
    "        other_points = X[labels != cid]\n",
    "        \n",
    "        if len(cluster_points) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For each feature, calculate separation\n",
    "        importance_scores = []\n",
    "        for f in range(X.shape[1]):\n",
    "            cluster_vals = cluster_points[:, f]\n",
    "            other_vals = other_points[:, f]\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            mean_diff = abs(cluster_vals.mean() - other_vals.mean())\n",
    "            pooled_std = np.sqrt((cluster_vals.std()**2 + other_vals.std()**2) / 2)\n",
    "            \n",
    "            if pooled_std > 0:\n",
    "                cohens_d = mean_diff / pooled_std\n",
    "            else:\n",
    "                cohens_d = 0\n",
    "            \n",
    "            importance_scores.append(cohens_d)\n",
    "        \n",
    "        feature_importance[cid] = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance_scores\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Get feature names (original features before UMAP)\n",
    "# You'll need to use your original feature columns here\n",
    "# For now, if you want to do this on UMAP dimensions:\n",
    "umap_feature_names = [f\"UMAP_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "feat_importance_train = calculate_feature_importance_for_assignment(\n",
    "    X_train, train_final, cent_train, umap_feature_names\n",
    ")\n",
    "\n",
    "print(\"\\nTop 3 Distinguishing Features per Cluster (Train Set):\")\n",
    "print(\"-\"*70)\n",
    "for cid in sorted(feat_importance_train.keys()):\n",
    "    print(f\"\\nCluster {cid}:\")\n",
    "    top_features = feat_importance_train[cid].head(3)\n",
    "    for _, row in top_features.iterrows():\n",
    "        print(f\"  {row['Feature']:<15} Importance: {row['Importance']:.3f}\")\n",
    "\n",
    "# For TEST set - check if feature importance pattern holds\n",
    "feat_importance_test = calculate_feature_importance_for_assignment(\n",
    "    X_test, test_final, cent_train, umap_feature_names\n",
    ")\n",
    "\n",
    "print(\"\\n\\nFeature Importance Consistency (Train vs Test):\")\n",
    "print(f\"{'Cluster':<8} {'Train Top Feat':<20} {'Test Top Feat':<20} {'Match?':<10}\")\n",
    "print(\"-\"*70)\n",
    "for cid in sorted(feat_importance_train.keys()):\n",
    "    if cid in feat_importance_test:\n",
    "        train_top = feat_importance_train[cid].iloc[0]['Feature']\n",
    "        test_top = feat_importance_test[cid].iloc[0]['Feature']\n",
    "        match = \"✓\" if train_top == test_top else \"✗\"\n",
    "        print(f\"{cid:<8} {train_top:<20} {test_top:<20} {match:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "635d45b9-1ee3-4c40-88a1-6914d8689ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Metric 4] Cluster Membership Stability\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Cluster Assignment Stability (Test Set):\n",
      "Cluster  Mean Stability     % Unstable (<0.7)    Status    \n",
      "----------------------------------------------------------------------\n",
      "0        0.690                31.5%  ❌ Fragile       ❌ Fragile\n",
      "1        1.000                 0.0%  ✓ Robust        ✓ Robust\n",
      "2        0.026                97.4%  ❌ Fragile       ❌ Fragile\n",
      "3        1.000                 0.0%  ✓ Robust        ✓ Robust\n",
      "4        1.000                 0.0%  ✓ Robust        ✓ Robust\n",
      "5        0.955                 6.5%  ✓ Robust        ✓ Robust\n",
      "6        1.000                 0.0%  ✓ Robust        ✓ Robust\n",
      "\n",
      "Overall Test Set Stability: 0.756\n",
      "Samples with stability < 0.7: 259 (24.7%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Metric 4] Cluster Membership Stability\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def calculate_stability_score(X, labels, centroids, n_bootstrap=50):\n",
    "    \"\"\"\n",
    "    For each point, see how often it gets assigned to the same cluster\n",
    "    when we add noise or bootstrap sample the data.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    stability_scores = np.zeros(n_samples)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Add small Gaussian noise\n",
    "        noise_std = 0.05 * X.std(axis=0)  # 5% noise\n",
    "        X_noisy = X + np.random.normal(0, noise_std, X.shape)\n",
    "        \n",
    "        # Reassign to nearest centroid\n",
    "        new_labels = np.array([\n",
    "            min(centroids.keys(), \n",
    "                key=lambda c: np.linalg.norm(X_noisy[i] - centroids[c]))\n",
    "            for i in range(n_samples)\n",
    "        ])\n",
    "        \n",
    "        # Check if assignment matches original\n",
    "        stability_scores += (new_labels == labels).astype(int)\n",
    "    \n",
    "    return stability_scores / n_bootstrap\n",
    "\n",
    "# Calculate stability for test set\n",
    "stability_test = calculate_stability_score(X_test, test_final, cent_train, n_bootstrap=50)\n",
    "\n",
    "# Summary by cluster\n",
    "stability_df = pd.DataFrame({\n",
    "    'cluster': test_final,\n",
    "    'stability': stability_test\n",
    "})\n",
    "\n",
    "print(\"\\nCluster Assignment Stability (Test Set):\")\n",
    "print(f\"{'Cluster':<8} {'Mean Stability':<18} {'% Unstable (<0.7)':<20} {'Status':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cid in sorted(stability_df['cluster'].unique()):\n",
    "    cluster_stab = stability_df[stability_df['cluster'] == cid]['stability']\n",
    "    mean_stab = cluster_stab.mean()\n",
    "    pct_unstable = (cluster_stab < 0.7).mean() * 100\n",
    "    \n",
    "    status = \"✓ Robust\" if pct_unstable < 10 else (\"⚠ Moderate\" if pct_unstable < 25 else \"❌ Fragile\")\n",
    "    print(f\"{cid:<8} {mean_stab:<18.3f} {pct_unstable:>6.1f}%  {status:<15} {status}\")\n",
    "\n",
    "print(f\"\\nOverall Test Set Stability: {stability_test.mean():.3f}\")\n",
    "print(f\"Samples with stability < 0.7: {(stability_test < 0.7).sum()} ({(stability_test < 0.7).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f676c0ea-ca5a-4a3f-a304-ffd69dbfa5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_patient_cluster(\n",
    "    patient_features,          # 1D numpy array of UMAP features\n",
    "    centroids,                 # dict {cluster_id: centroid array}\n",
    "    feature_names,             # list of feature names (UMAP1, UMAP2, ...)\n",
    "    feature_importance_dict,   # dict {cluster_id: pd.DataFrame with Feature, Importance}\n",
    "    stability_score=None       # optional stability score for this patient\n",
    "):\n",
    "    # 1️⃣ Compute distances to all centroids\n",
    "    distances = {cid: np.linalg.norm(patient_features - c) for cid, c in centroids.items()}\n",
    "    \n",
    "    # Assigned cluster: nearest centroid\n",
    "    assigned_cluster = min(distances, key=distances.get)\n",
    "    distance_to_assigned = distances[assigned_cluster]\n",
    "    \n",
    "    # Nearest competing cluster\n",
    "    other_distances = {cid: d for cid, d in distances.items() if cid != assigned_cluster}\n",
    "    nearest_other_cluster = min(other_distances, key=other_distances.get)\n",
    "    distance_to_other = other_distances[nearest_other_cluster]\n",
    "    \n",
    "    # Separation ratio\n",
    "    separation_ratio = (distance_to_other - distance_to_assigned) / distance_to_assigned if distance_to_assigned > 0 else np.inf\n",
    "    \n",
    "    # 2️⃣ Top features pulling patient toward assigned cluster\n",
    "    # Use feature_importance_dict from Metric 3 (cohen's d per feature)\n",
    "    importance_df = feature_importance_dict.get(assigned_cluster, pd.DataFrame())\n",
    "    if not importance_df.empty:\n",
    "        # Match features with patient differences from centroid\n",
    "        contributions = {}\n",
    "        for _, row in importance_df.iterrows():\n",
    "            feat = row['Feature']\n",
    "            # Absolute difference from assigned cluster centroid\n",
    "            feat_idx = feature_names.index(feat)\n",
    "            contributions[feat] = abs(patient_features[feat_idx] - centroids[assigned_cluster][feat_idx]) * row['Importance']\n",
    "        # Top 3 features\n",
    "        top_features = sorted(contributions.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        top_features_list = [{'Feature': f, 'Contribution': round(c, 3)} for f, c in top_features]\n",
    "    else:\n",
    "        top_features_list = []\n",
    "\n",
    "    # 3️⃣ Construct summary\n",
    "    summary_text = (\n",
    "        f\"Patient is assigned to Cluster {assigned_cluster} because they are \"\n",
    "        f\"{separation_ratio:.2f} separation ratio closer in UMAP space compared to Cluster {nearest_other_cluster}. \"\n",
    "    )\n",
    "    if top_features_list:\n",
    "        feat_names = ', '.join([f['Feature'] for f in top_features_list])\n",
    "        summary_text += f\"Key features driving this assignment include {feat_names}. \"\n",
    "    if stability_score is not None:\n",
    "        summary_text += f\"Cluster membership stability = {stability_score:.2f}.\"\n",
    "    \n",
    "    # 4️⃣ Return structured explanation\n",
    "    return {\n",
    "        'assigned_cluster': assigned_cluster,\n",
    "        'nearest_other_cluster': nearest_other_cluster,\n",
    "        'distance_to_assigned': distance_to_assigned,\n",
    "        'distance_to_other': distance_to_other,\n",
    "        'separation_ratio': separation_ratio,\n",
    "        'top_features_pulling_to_assigned': top_features_list,\n",
    "        'stability_score': stability_score,\n",
    "        'summary': summary_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "924f3e9b-3005-4e2f-83f0-424a034a3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_umap_feature_map(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    umap_map = {}\n",
    "\n",
    "    # For UMAP1 to UMAP20\n",
    "    for i in range(1, 21):\n",
    "        feature_col = f\"UMAP{i}_Feature\"\n",
    "        corr_col = f\"UMAP{i}_Correlation\"\n",
    "\n",
    "        if feature_col in df.columns and corr_col in df.columns:\n",
    "            # Always take the FIRST row = highest-correlation feature\n",
    "            best_feature = df.loc[0, feature_col]\n",
    "            best_corr = df.loc[0, corr_col]\n",
    "\n",
    "            # Store in normalized form \"UMAP_i\"\n",
    "            umap_map[f\"UMAP_{i}\"] = {\n",
    "                \"feature\": best_feature,\n",
    "                \"correlation\": float(best_corr)\n",
    "            }\n",
    "\n",
    "    return umap_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e74d0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_umap_to_original(result, umap_map):\n",
    "    translated = []\n",
    "\n",
    "    for item in result[\"top_features_pulling_to_assigned\"]:\n",
    "        umap_feature = item[\"Feature\"]  # like \"UMAP_8\"\n",
    "        contrib = item[\"Contribution\"]\n",
    "\n",
    "        if umap_feature in umap_map:\n",
    "            translated.append({\n",
    "                \"Original_Feature\": umap_map[umap_feature][\"feature\"],\n",
    "                \"Correlation\": umap_map[umap_feature][\"correlation\"],\n",
    "                \"Contribution\": contrib\n",
    "            })\n",
    "        else:\n",
    "            print(f\"[Warning] No mapped feature for {umap_feature}\")\n",
    "\n",
    "    # sort by contribution descending\n",
    "    translated = sorted(translated, key=lambda x: x[\"Contribution\"], reverse=True)\n",
    "    return translated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1817a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_summary(result, original_features):\n",
    "    if original_features:\n",
    "        top_feats = \", \".join([f[\"Original_Feature\"] for f in original_features[:3]])\n",
    "    else:\n",
    "        top_feats = \"N/A\"\n",
    "\n",
    "    result[\"summary\"] = (\n",
    "        f\"Patient is assigned to Cluster {result['assigned_cluster']} because they are \"\n",
    "        f\"{result['separation_ratio']:.2f} separation ratio closer in UMAP space compared to Cluster \"\n",
    "        f\"{result['nearest_other_cluster']}. Key original health features driving this assignment include \"\n",
    "        f\"{top_feats}. Cluster membership stability = {result['stability_score']:.2f}.\"\n",
    "    )\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58f60c",
   "metadata": {},
   "source": [
    "## Test Patient 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06761448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assigned_cluster': 0,\n",
      " 'distance_to_assigned': 0.666987075804459,\n",
      " 'distance_to_other': 1.8968030256899717,\n",
      " 'nearest_other_cluster': 5,\n",
      " 'separation_ratio': 1.8438377511321653,\n",
      " 'stability_score': 1.0,\n",
      " 'summary': 'Patient is assigned to Cluster 0 because they are 1.84 separation '\n",
      "            'ratio closer in UMAP space compared to Cluster 5. Key features '\n",
      "            'driving this assignment include UMAP_1, UMAP_8, UMAP_15. Cluster '\n",
      "            'membership stability = 1.00.',\n",
      " 'top_features_pulling_to_assigned': [{'Contribution': 0.321,\n",
      "                                       'Feature': 'UMAP_1'},\n",
      "                                      {'Contribution': 0.246,\n",
      "                                       'Feature': 'UMAP_8'},\n",
      "                                      {'Contribution': 0.245,\n",
      "                                       'Feature': 'UMAP_15'}]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage for first patient in test set\n",
    "patient_idx = 0\n",
    "patient_features = X_test[patient_idx]\n",
    "stability = stability_test[patient_idx]\n",
    "explanation = explain_patient_cluster(\n",
    "    patient_features,\n",
    "    cent_train,\n",
    "    umap_feature_names,\n",
    "    feat_importance_train,\n",
    "    stability_score=stability\n",
    ")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ce6dcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assigned_cluster': 0,\n",
       " 'nearest_other_cluster': 5,\n",
       " 'distance_to_assigned': 0.666987075804459,\n",
       " 'distance_to_other': 1.8968030256899717,\n",
       " 'separation_ratio': 1.8438377511321653,\n",
       " 'top_features_pulling_to_assigned': [{'Feature': 'UMAP_1',\n",
       "   'Contribution': 0.321},\n",
       "  {'Feature': 'UMAP_8', 'Contribution': 0.246},\n",
       "  {'Feature': 'UMAP_15', 'Contribution': 0.245}],\n",
       " 'stability_score': 1.0,\n",
       " 'summary': 'Patient is assigned to Cluster 0 because they are 1.84 separation ratio closer in UMAP space compared to Cluster 5. Key original health features driving this assignment include Has_Kidney_Failure, mean_steroid_ng_dl, Has_Kidney_Failure. Cluster membership stability = 1.00.',\n",
       " 'top_original_features': [{'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.964724305029977,\n",
       "   'Contribution': 0.321},\n",
       "  {'Original_Feature': 'mean_steroid_ng_dl',\n",
       "   'Correlation': 0.4647739244486011,\n",
       "   'Contribution': 0.246},\n",
       "  {'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.9584534157393116,\n",
       "   'Contribution': 0.245}]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_map = load_umap_feature_map(\"umap_health_features_with_correlations.csv\")\n",
    "\n",
    "orig_features = translate_umap_to_original(explanation, umap_map)\n",
    "\n",
    "explanation[\"top_original_features\"] = orig_features\n",
    "\n",
    "explanation = update_summary(explanation, orig_features)\n",
    "\n",
    "explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784bf7e",
   "metadata": {},
   "source": [
    "## Test Patient 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32660818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assigned_cluster': 0,\n",
      " 'distance_to_assigned': 1.1707468278750008,\n",
      " 'distance_to_other': 1.3512790845628408,\n",
      " 'nearest_other_cluster': 5,\n",
      " 'separation_ratio': 0.15420264432022462,\n",
      " 'stability_score': 0.94,\n",
      " 'summary': 'Patient is assigned to Cluster 0 because they are 0.15 separation '\n",
      "            'ratio closer in UMAP space compared to Cluster 5. Key features '\n",
      "            'driving this assignment include UMAP_18, UMAP_7, UMAP_19. Cluster '\n",
      "            'membership stability = 0.94.',\n",
      " 'top_features_pulling_to_assigned': [{'Contribution': 0.491,\n",
      "                                       'Feature': 'UMAP_18'},\n",
      "                                      {'Contribution': 0.396,\n",
      "                                       'Feature': 'UMAP_7'},\n",
      "                                      {'Contribution': 0.361,\n",
      "                                       'Feature': 'UMAP_19'}]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage for first patient in test set\n",
    "patient_idx = 10\n",
    "patient_features = X_test[patient_idx]\n",
    "stability = stability_test[patient_idx]\n",
    "explanation = explain_patient_cluster(\n",
    "    patient_features,\n",
    "    cent_train,\n",
    "    umap_feature_names,\n",
    "    feat_importance_train,\n",
    "    stability_score=stability\n",
    ")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac93f6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assigned_cluster': 0,\n",
       " 'nearest_other_cluster': 5,\n",
       " 'distance_to_assigned': 1.1707468278750008,\n",
       " 'distance_to_other': 1.3512790845628408,\n",
       " 'separation_ratio': 0.15420264432022462,\n",
       " 'top_features_pulling_to_assigned': [{'Feature': 'UMAP_18',\n",
       "   'Contribution': 0.491},\n",
       "  {'Feature': 'UMAP_7', 'Contribution': 0.396},\n",
       "  {'Feature': 'UMAP_19', 'Contribution': 0.361}],\n",
       " 'stability_score': 0.94,\n",
       " 'summary': 'Patient is assigned to Cluster 0 because they are 0.15 separation ratio closer in UMAP space compared to Cluster 5. Key original health features driving this assignment include Has_Kidney_Failure, Has_Kidney_Failure, Has_Kidney_Failure. Cluster membership stability = 0.94.',\n",
       " 'top_original_features': [{'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.7471517552457481,\n",
       "   'Contribution': 0.491},\n",
       "  {'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.9647737840032344,\n",
       "   'Contribution': 0.396},\n",
       "  {'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.9179585731525768,\n",
       "   'Contribution': 0.361}]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_map = load_umap_feature_map(\"umap_health_features_with_correlations.csv\")\n",
    "\n",
    "orig_features = translate_umap_to_original(explanation, umap_map)\n",
    "\n",
    "explanation[\"top_original_features\"] = orig_features\n",
    "\n",
    "explanation = update_summary(explanation, orig_features)\n",
    "\n",
    "explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb815e9",
   "metadata": {},
   "source": [
    "## Test Patient 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "96e7d03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assigned_cluster': 4,\n",
      " 'distance_to_assigned': 0.869120492545926,\n",
      " 'distance_to_other': 3.4356442763437847,\n",
      " 'nearest_other_cluster': 5,\n",
      " 'separation_ratio': 2.9530126211610854,\n",
      " 'stability_score': 1.0,\n",
      " 'summary': 'Patient is assigned to Cluster 4 because they are 2.95 separation '\n",
      "            'ratio closer in UMAP space compared to Cluster 5. Key features '\n",
      "            'driving this assignment include UMAP_17, UMAP_8, UMAP_11. Cluster '\n",
      "            'membership stability = 1.00.',\n",
      " 'top_features_pulling_to_assigned': [{'Contribution': 0.485,\n",
      "                                       'Feature': 'UMAP_17'},\n",
      "                                      {'Contribution': 0.467,\n",
      "                                       'Feature': 'UMAP_8'},\n",
      "                                      {'Contribution': 0.237,\n",
      "                                       'Feature': 'UMAP_11'}]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage for first patient in test set\n",
    "patient_idx = 47\n",
    "patient_features = X_test[patient_idx]\n",
    "stability = stability_test[patient_idx]\n",
    "explanation = explain_patient_cluster(\n",
    "    patient_features,\n",
    "    cent_train,\n",
    "    umap_feature_names,\n",
    "    feat_importance_train,\n",
    "    stability_score=stability\n",
    ")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df6bbc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assigned_cluster': 4,\n",
       " 'nearest_other_cluster': 5,\n",
       " 'distance_to_assigned': 0.869120492545926,\n",
       " 'distance_to_other': 3.4356442763437847,\n",
       " 'separation_ratio': 2.9530126211610854,\n",
       " 'top_features_pulling_to_assigned': [{'Feature': 'UMAP_17',\n",
       "   'Contribution': 0.485},\n",
       "  {'Feature': 'UMAP_8', 'Contribution': 0.467},\n",
       "  {'Feature': 'UMAP_11', 'Contribution': 0.237}],\n",
       " 'stability_score': 1.0,\n",
       " 'summary': 'Patient is assigned to Cluster 4 because they are 2.95 separation ratio closer in UMAP space compared to Cluster 5. Key original health features driving this assignment include Has_Kidney_Failure, mean_steroid_ng_dl, blood_macros. Cluster membership stability = 1.00.',\n",
       " 'top_original_features': [{'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.9412905841920932,\n",
       "   'Contribution': 0.485},\n",
       "  {'Original_Feature': 'mean_steroid_ng_dl',\n",
       "   'Correlation': 0.4647739244486011,\n",
       "   'Contribution': 0.467},\n",
       "  {'Original_Feature': 'blood_macros',\n",
       "   'Correlation': 0.323578178194059,\n",
       "   'Contribution': 0.237}]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_map = load_umap_feature_map(\"umap_health_features_with_correlations.csv\")\n",
    "\n",
    "orig_features = translate_umap_to_original(explanation, umap_map)\n",
    "\n",
    "explanation[\"top_original_features\"] = orig_features\n",
    "\n",
    "explanation = update_summary(explanation, orig_features)\n",
    "\n",
    "explanation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
