{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3d6f145b-fd39-4698-b4f6-5df89d224e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "753bbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def preprocess_new_record(new_record, pipeline_path='Data/preprocessing_pipeline.pkl'):\n",
    "    \"\"\"\n",
    "    Apply the same preprocessing to new records\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    new_record : dict or pd.DataFrame\n",
    "        New record(s) to preprocess\n",
    "    pipeline_path : str\n",
    "        Path to saved preprocessing pipeline\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Preprocessed record matching training data format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pipeline\n",
    "    with open(pipeline_path, 'rb') as f:\n",
    "        pipeline = pickle.load(f)\n",
    "    \n",
    "    # Convert to DataFrame if needed\n",
    "    if isinstance(new_record, dict):\n",
    "        df_new = pd.DataFrame([new_record])\n",
    "    else:\n",
    "        df_new = new_record.copy()\n",
    "    \n",
    "    # Step 1: Normalize Unknown-like responses\n",
    "    df_new = df_new.replace({\n",
    "        \"Don't know\": \"Unknown\", \"Refused\": \"Unknown\", \n",
    "        \"Not Applicable\": \"Unknown\", \"N/A\": \"Unknown\", \n",
    "        \"Unknown/NA\": \"Unknown\"\n",
    "    })\n",
    "    \n",
    "    # Step 2: Binary encoding\n",
    "    for col in pipeline['binary_cols']:\n",
    "        if col not in df_new.columns:\n",
    "            continue\n",
    "        \n",
    "        if col == \"Has_diabetes\":\n",
    "            mapping = pipeline['binary_mappings'][\"Has_diabetes\"]\n",
    "        elif col == \"Received_Hepatitis_A_Vaccine\":\n",
    "            mapping = pipeline['binary_mappings'][\"Received_Hepatitis_A_Vaccine\"]\n",
    "        else:\n",
    "            mapping = pipeline['binary_mappings'][\"default\"]\n",
    "        \n",
    "        df_new[col] = df_new[col].map(mapping)\n",
    "    \n",
    "    # Step 3: Ordinal encoding\n",
    "    for col, encoder in pipeline['ordinal_encoders'].items():\n",
    "        if col in df_new.columns:\n",
    "            df_new[col] = encoder.transform(df_new[[col]])\n",
    "    \n",
    "    # Step 4: One-hot encoding\n",
    "    for base_col in pipeline['ohe_cols']:\n",
    "        if base_col in df_new.columns:\n",
    "            # Get dummies for this column\n",
    "            dummies = pd.get_dummies(df_new[base_col], prefix=base_col, dtype=int)\n",
    "            \n",
    "            # Add any missing columns from training\n",
    "            for train_col in pipeline['ohe_column_names']:\n",
    "                if base_col in train_col and train_col not in dummies.columns:\n",
    "                    dummies[train_col] = 0\n",
    "            \n",
    "            # Remove extra columns not in training\n",
    "            cols_to_keep = [col for col in dummies.columns \n",
    "                           if col in pipeline['ohe_column_names']]\n",
    "            dummies = dummies[cols_to_keep]\n",
    "            \n",
    "            # Add to dataframe\n",
    "            df_new = pd.concat([df_new.drop(columns=[base_col]), dummies], axis=1)\n",
    "    \n",
    "    # Step 5: Apply log transformation to skewed columns\n",
    "    for col in pipeline['skewed_cols']:\n",
    "        if col in df_new.columns:\n",
    "            df_new[col] = np.log1p(df_new[col].clip(lower=0))\n",
    "    \n",
    "    # Step 6: Ensure all columns from training exist\n",
    "    for col in pipeline['all_columns']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 0  # Add missing columns with default value\n",
    "    \n",
    "    # Step 7: Reorder columns to match training data\n",
    "    df_new = df_new[pipeline['all_columns']]\n",
    "    \n",
    "    # Step 8: Apply scaling\n",
    "    df_new[pipeline['cols_to_scale']] = pipeline['scaler'].transform(\n",
    "        df_new[pipeline['cols_to_scale']]\n",
    "    )\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a2c1d87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of processed record: (1050, 55)\n",
      "Matches training data shape: True\n"
     ]
    }
   ],
   "source": [
    "# Example new patient record\n",
    "test_df = pd.read_csv('Data/test_dataset.csv')\n",
    "df_scaled = pd.read_csv('Data/df_scaled.csv')\n",
    "df_scaled.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "# Preprocess the new record\n",
    "processed_patient = preprocess_new_record(test_df)\n",
    "\n",
    "print(f\"Shape of processed record: {processed_patient.shape}\")\n",
    "print(f\"Matches training data shape: {processed_patient.shape[1] == df_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee0418",
   "metadata": {},
   "source": [
    "# Dim Reduction UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "60e68930-e6bd-4abc-ad7d-4d0cda42542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5f78dd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 22 19:48:11 2025 Building and compiling search function\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c4ae07273645b0bcb7808adf6177ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/100 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  100 epochs\n",
      "\tcompleted  10  /  100 epochs\n",
      "\tcompleted  20  /  100 epochs\n",
      "\tcompleted  30  /  100 epochs\n",
      "\tcompleted  40  /  100 epochs\n",
      "\tcompleted  50  /  100 epochs\n",
      "\tcompleted  60  /  100 epochs\n",
      "\tcompleted  70  /  100 epochs\n",
      "\tcompleted  80  /  100 epochs\n",
      "\tcompleted  90  /  100 epochs\n",
      "(1050, 20)\n",
      "       UMAP1      UMAP2     UMAP3     UMAP4     UMAP5     UMAP6     UMAP7  \\\n",
      "0  13.024805  13.073879  5.157461  2.991746  5.764147  5.317441  6.363474   \n",
      "1  -4.150143   5.798073  5.107178  6.727241  4.209681  2.940264 -1.061531   \n",
      "2  12.650412  12.129565  5.002039  3.329209  5.700506  6.181603  5.701279   \n",
      "3  12.410887   9.398935  4.929136  3.380293  5.901017  5.174147  5.899176   \n",
      "4  -4.033573   5.764928  5.089195  6.560227  4.229160  2.989349 -0.971349   \n",
      "\n",
      "      UMAP8     UMAP9    UMAP10    UMAP11    UMAP12    UMAP13    UMAP14  \\\n",
      "0  6.094333  1.970684  3.853272  6.677246  4.087493  3.857002  2.361790   \n",
      "1  5.780765  7.538804  4.759531  6.841758  3.087821  8.424713  8.717398   \n",
      "2  7.108046  4.273507  3.404667  7.138408  4.130431  3.944384  3.475381   \n",
      "3  5.939536  2.063258  3.597143  6.700058  4.690133  3.555218  3.359969   \n",
      "4  5.759371  7.471111  4.778776  6.771988  2.911294  8.289840  8.615449   \n",
      "\n",
      "     UMAP15    UMAP16    UMAP17    UMAP18    UMAP19    UMAP20  \n",
      "0  2.919640  2.792840  6.533456  4.453222  0.049239  1.572855  \n",
      "1  5.998413  4.511382  3.324929  3.264351  4.680604  5.339237  \n",
      "2  3.410046  3.692559  5.797626  4.835593  0.250794  1.935026  \n",
      "3  2.902275  3.136115  6.473524  5.662247  0.888806  2.056911  \n",
      "4  6.082884  4.492987  3.323495  3.305792  4.667191  5.574940  \n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "umap = joblib.load('Data/umap_model.pkl')\n",
    "umap_test = umap.transform(processed_patient)\n",
    "umap_test_df = pd.DataFrame(umap_test, columns=[f'UMAP{i+1}' for i in range(umap_test.shape[1])])\n",
    "umap_test_df.head()\n",
    "\n",
    "print(umap_test_df.shape)\n",
    "print(umap_test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0b709dc8-f2a0-4669-826d-1c746ecc86ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP1     float32\n",
      "UMAP2     float32\n",
      "UMAP3     float32\n",
      "UMAP4     float32\n",
      "UMAP5     float32\n",
      "UMAP6     float32\n",
      "UMAP7     float32\n",
      "UMAP8     float32\n",
      "UMAP9     float32\n",
      "UMAP10    float32\n",
      "UMAP11    float32\n",
      "UMAP12    float32\n",
      "UMAP13    float32\n",
      "UMAP14    float32\n",
      "UMAP15    float32\n",
      "UMAP16    float32\n",
      "UMAP17    float32\n",
      "UMAP18    float32\n",
      "UMAP19    float32\n",
      "UMAP20    float32\n",
      "dtype: object\n",
      "UMAP1     float64\n",
      "UMAP2     float64\n",
      "UMAP3     float64\n",
      "UMAP4     float64\n",
      "UMAP5     float64\n",
      "UMAP6     float64\n",
      "UMAP7     float64\n",
      "UMAP8     float64\n",
      "UMAP9     float64\n",
      "UMAP10    float64\n",
      "UMAP11    float64\n",
      "UMAP12    float64\n",
      "UMAP13    float64\n",
      "UMAP14    float64\n",
      "UMAP15    float64\n",
      "UMAP16    float64\n",
      "UMAP17    float64\n",
      "UMAP18    float64\n",
      "UMAP19    float64\n",
      "UMAP20    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(umap_test_df.dtypes)\n",
    "umap_test_df = umap_test_df.astype('float64')\n",
    "print(umap_test_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5b1d0",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2dace-764a-4e05-91d0-3eab8b1aba94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "50a75596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ASSIGNING FINAL CLUSTERS FOR RECOMMENDATION SYSTEM\n",
      "======================================================================\n",
      "✓ Models loaded:\n",
      "  - kmeans_final (initial 4 clusters)\n",
      "  - kmeans_c0 (split for cluster 0)\n",
      "  - kmeans_c3 (split for cluster 3)\n",
      "\n",
      "Initial cluster assignments:\n",
      "0    341\n",
      "1    173\n",
      "2    155\n",
      "3    381\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final cluster assignments:\n",
      "Final_Cluster_UMAP\n",
      "0    126\n",
      "1    155\n",
      "2     60\n",
      "3    173\n",
      "4    155\n",
      "5    201\n",
      "6    180\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample (first 20 rows):\n",
      "    Cluster_Original  Subcluster_0  Subcluster_3  Final_Cluster_UMAP\n",
      "0                  3            -1             1                   6\n",
      "1                  1            -1            -1                   3\n",
      "2                  0             0            -1                   0\n",
      "3                  2            -1            -1                   4\n",
      "4                  1            -1            -1                   3\n",
      "5                  3            -1             0                   5\n",
      "6                  2            -1            -1                   4\n",
      "7                  3            -1             0                   5\n",
      "8                  1            -1            -1                   3\n",
      "9                  2            -1            -1                   4\n",
      "10                 3            -1             1                   6\n",
      "11                 3            -1             0                   5\n",
      "12                 0             1            -1                   1\n",
      "13                 3            -1             1                   6\n",
      "14                 1            -1            -1                   3\n",
      "15                 2            -1            -1                   4\n",
      "16                 0             0            -1                   0\n",
      "17                 0             1            -1                   1\n",
      "18                 0             1            -1                   1\n",
      "19                 2            -1            -1                   4\n",
      "\n",
      "✓ Results saved to: Data/test_patients_with_clusters_kmeans_umap.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ASSIGN FINAL CLUSTERS FOR RECOMMENDATION SYSTEM (VS STEP 7 STYLE)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ASSIGNING FINAL CLUSTERS FOR RECOMMENDATION SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Load trained clustering models (same ones used in VS pipeline)\n",
    "# ------------------------------------------------------------------\n",
    "kmeans_final = joblib.load('Models/kmeans_umap_initial4_model.pkl')  # initial 4-cluster KMeans\n",
    "kmeans_c0    = joblib.load('Models/kmeans_umap_c0.pkl')              # subcluster model for cluster 0\n",
    "kmeans_c3    = joblib.load('Models/kmeans_umap_c3.pkl')              # subcluster model for cluster 3\n",
    "\n",
    "print(\"✓ Models loaded:\")\n",
    "print(\"  - kmeans_final (initial 4 clusters)\")\n",
    "print(\"  - kmeans_c0 (split for cluster 0)\")\n",
    "print(\"  - kmeans_c3 (split for cluster 3)\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Predict initial clusters (0,1,2,3)\n",
    "# ------------------------------------------------------------------\n",
    "umap_only = umap_test_df.filter(regex=\"^UMAP\").astype(\"float64\")\n",
    "initial_clusters = kmeans_final.predict(umap_only.values)\n",
    "\n",
    "umap_test_df[\"Cluster_Original\"] = initial_clusters\n",
    "umap_test_df[\"Cluster_Refined\"] = initial_clusters\n",
    "\n",
    "print(\"\\nInitial cluster assignments:\")\n",
    "print(pd.Series(initial_clusters).value_counts().sort_index())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Compute subclusters for 0 and 3\n",
    "# ------------------------------------------------------------------\n",
    "sub_labels_c0 = np.full(len(umap_test_df), -1)\n",
    "sub_labels_c3 = np.full(len(umap_test_df), -1)\n",
    "\n",
    "# cluster 0 → 3 subclusters (0,1,2)\n",
    "idx0 = np.where(initial_clusters == 0)[0]\n",
    "if len(idx0) > 0:\n",
    "    sub_labels_c0[idx0] = kmeans_c0.predict(umap_only.iloc[idx0].values)\n",
    "\n",
    "# cluster 3 → 2 subclusters (0,1)\n",
    "idx3 = np.where(initial_clusters == 3)[0]\n",
    "if len(idx3) > 0:\n",
    "    sub_labels_c3[idx3] = kmeans_c3.predict(umap_only.iloc[idx3].values)\n",
    "\n",
    "# Save to df for inspection\n",
    "umap_test_df[\"Subcluster_0\"] = sub_labels_c0\n",
    "umap_test_df[\"Subcluster_3\"] = sub_labels_c3\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. REFINED LABEL CREATION (IDENTICAL TO VS STEP 7)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "new_cluster_id = 0\n",
    "\n",
    "# ----------------------\n",
    "# Cluster 0 → split\n",
    "# ----------------------\n",
    "for idx in idx0:\n",
    "    umap_test_df.loc[idx, \"Cluster_Refined\"] = new_cluster_id + sub_labels_c0[idx]\n",
    "new_cluster_id += 3      # (0,1,2)\n",
    "\n",
    "# ----------------------\n",
    "# Cluster 1 → single block\n",
    "# ----------------------\n",
    "idx1 = np.where(initial_clusters == 1)[0]\n",
    "umap_test_df.loc[idx1, \"Cluster_Refined\"] = new_cluster_id\n",
    "new_cluster_id += 1      # now 4\n",
    "\n",
    "# ----------------------\n",
    "# Cluster 2 → single block\n",
    "# ----------------------\n",
    "idx2 = np.where(initial_clusters == 2)[0]\n",
    "umap_test_df.loc[idx2, \"Cluster_Refined\"] = new_cluster_id\n",
    "new_cluster_id += 1      # now 5\n",
    "\n",
    "# ----------------------\n",
    "# Cluster 3 → split\n",
    "# ----------------------\n",
    "for idx in idx3:\n",
    "    umap_test_df.loc[idx, \"Cluster_Refined\"] = new_cluster_id + sub_labels_c3[idx]\n",
    "new_cluster_id += 2      # 5,6 → now 7\n",
    "\n",
    "# finalize\n",
    "umap_test_df[\"Final_Cluster_UMAP\"] = umap_test_df[\"Cluster_Refined\"]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Verification\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\nFinal cluster assignments:\")\n",
    "print(umap_test_df[\"Final_Cluster_UMAP\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nSample (first 20 rows):\")\n",
    "print(umap_test_df[[\n",
    "    \"Cluster_Original\",\n",
    "    \"Subcluster_0\",\n",
    "    \"Subcluster_3\",\n",
    "    \"Final_Cluster_UMAP\"\n",
    "]].head(20))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. Save results\n",
    "# ------------------------------------------------------------------\n",
    "output_path = \"Data/test_patients_with_clusters_kmeans_umap.csv\"\n",
    "umap_test_df.to_csv(output_path, index=False)\n",
    "print(f\"\\n✓ Results saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58852f1",
   "metadata": {},
   "source": [
    "# Cluster Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ca8fd7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLUSTER ASSIGNMENT VALIDATION\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLUSTER ASSIGNMENT VALIDATION\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3a677e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded: (9442, 20)\n",
      "Columns: ['UMAP1', 'UMAP2', 'UMAP3', 'UMAP4', 'UMAP5', 'UMAP6', 'UMAP7', 'UMAP8', 'UMAP9', 'UMAP10', 'UMAP11', 'UMAP12', 'UMAP13', 'UMAP14', 'UMAP15', 'UMAP16', 'UMAP17', 'UMAP18', 'UMAP19', 'UMAP20']\n"
     ]
    }
   ],
   "source": [
    "# Load the training PCA data\n",
    "df_umap = pd.read_csv('Data/umap_dataset.csv')\n",
    "\n",
    "print(f\"Training data loaded: {df_umap.shape}\")\n",
    "print(f\"Columns: {df_umap.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2d60d288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Metric 1] Within-Cluster Variance Stability\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Within-cluster variance comparison (Initial 4 clusters):\n",
      "Cluster    Train Variance       Test Variance        % Change       \n",
      "----------------------------------------------------------------------\n",
      "0          2.0927               2.1384               +2.18%  ✓ Stable\n",
      "1          1.6836               2.0496               +21.74%  ⚠ Check\n",
      "2          0.9030               0.9252               +2.46%  ✓ Stable\n",
      "3          1.5496               1.4980               -3.33%  ✓ Stable\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Metric 1] Within-Cluster Variance Stability\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def calculate_within_cluster_variance(data, labels, centroids):\n",
    "    \"\"\"Calculate within-cluster variance for each cluster\"\"\"\n",
    "    variances = {}\n",
    "    for cluster_id in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster_id]\n",
    "        centroid = centroids[cluster_id]\n",
    "        variance = np.mean(np.sum((cluster_points - centroid)**2, axis=1))\n",
    "        variances[cluster_id] = variance\n",
    "    return variances\n",
    "\n",
    "# For initial 4 clusters - calculate on TRAINING data\n",
    "train_initial_labels = kmeans_final.labels_  # Training labels\n",
    "train_centroids = kmeans_final.cluster_centers_\n",
    "\n",
    "# Calculate training variance\n",
    "train_variances = calculate_within_cluster_variance(\n",
    "    df_umap.drop(columns=['Cluster', 'Cluster_Original', 'Cluster_Refined'], errors='ignore').values,\n",
    "    train_initial_labels,\n",
    "    train_centroids\n",
    ")\n",
    "\n",
    "# Calculate test variance\n",
    "test_initial_labels = initial_clusters\n",
    "# ✅ Use correct object (umap_test_df if DataFrame, else pca_test if array)\n",
    "umap_cols = [f\"UMAP{i}\" for i in range(1, 21)]\n",
    "test_data = umap_test_df[umap_cols].to_numpy(dtype=np.float32, copy=True)\n",
    "\n",
    "test_variances = calculate_within_cluster_variance(\n",
    "    test_data,\n",
    "    test_initial_labels,\n",
    "    train_centroids\n",
    ")\n",
    "\n",
    "print(\"\\nWithin-cluster variance comparison (Initial 4 clusters):\")\n",
    "print(f\"{'Cluster':<10} {'Train Variance':<20} {'Test Variance':<20} {'% Change':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cluster_id in sorted(train_variances.keys()):\n",
    "    train_var = train_variances[cluster_id]\n",
    "    test_var = test_variances.get(cluster_id, np.nan)\n",
    "    \n",
    "    if not np.isnan(test_var):\n",
    "        pct_change = ((test_var - train_var) / train_var) * 100\n",
    "        status = \"✓ Stable\" if abs(pct_change) < 20 else \"⚠ Check\"\n",
    "        print(f\"{cluster_id:<10} {train_var:<20.4f} {test_var:<20.4f} {pct_change:>+.2f}%  {status}\")\n",
    "    else:\n",
    "        print(f\"{cluster_id:<10} {train_var:<20.4f} {'No test samples':<20} {'N/A':<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5ca1fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Within-cluster variance comparison (Final UMAP clusters):\n",
      "Cluster  Train Var          Test Var           % Change    \n",
      "------------------------------------------------------------\n",
      "0        1.1721             0.7820               -33.28%  ⚠ Check\n",
      "1        0.7008             0.7031                +0.32%  ✓ Stable\n",
      "2        0.5650             0.7824               +38.47%  ⚠ Check\n",
      "3        1.6836             2.0318               +20.69%  ✓ Stable\n",
      "4        0.9030             0.9156                +1.39%  ✓ Stable\n",
      "5        0.6893             0.7185                +4.23%  ✓ Stable\n",
      "6        1.4584             1.4219                -2.50%  ✓ Stable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_centroids_and_variances(X, labels):\n",
    "    \"\"\"Return centroids and within-cluster variances for given labels.\"\"\"\n",
    "    centroids = {}\n",
    "    variances = {}\n",
    "    for cid in np.unique(labels):\n",
    "        mask = (labels == cid)\n",
    "        pts = X[mask]\n",
    "        if pts.size == 0:\n",
    "            continue\n",
    "        c = pts.mean(axis=0)\n",
    "        centroids[cid] = c\n",
    "        d2 = np.sum((pts - c)**2, axis=1)\n",
    "        variances[cid] = d2.mean()\n",
    "    return centroids, variances\n",
    "\n",
    "# ============================================================\n",
    "# BUILD TRAIN UMAP MATRIX + FINAL CLUSTER LABELS\n",
    "# ============================================================\n",
    "umap_cols = [c for c in df_umap.columns if c.startswith(\"UMAP\")]\n",
    "train_umap_df = df_umap[umap_cols]\n",
    "\n",
    "# Ensure same column order as used to train kmeans_final\n",
    "if hasattr(kmeans_final, \"feature_names_in_\"):\n",
    "    expected_cols = list(kmeans_final.feature_names_in_)\n",
    "    train_umap_df = train_umap_df[expected_cols]\n",
    "\n",
    "# Match dtype to model\n",
    "X_train = train_umap_df.to_numpy().astype(kmeans_final.cluster_centers_.dtype)\n",
    "train_initial = kmeans_final.labels_  # 0..3 on train\n",
    "\n",
    "train_final = np.empty_like(train_initial)\n",
    "\n",
    "for i, main in enumerate(train_initial):\n",
    "    point = X_train[i:i+1, :]  # shape (1, d)\n",
    "    \n",
    "    if main == 0:\n",
    "        # initial 0 → subclusters 0,1,2 → final 0,1,2\n",
    "        sub = kmeans_c0.predict(point)[0]      # 0,1,2\n",
    "        final = 0 + sub\n",
    "    elif main == 1:\n",
    "        # initial 1 → final 3\n",
    "        final = 3\n",
    "    elif main == 2:\n",
    "        # initial 2 → final 4\n",
    "        final = 4\n",
    "    elif main == 3:\n",
    "        # initial 3 → subclusters 0,1 → final 5,6\n",
    "        sub = kmeans_c3.predict(point)[0]      # 0,1\n",
    "        final = 5 + sub\n",
    "    \n",
    "    train_final[i] = int(final)\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN VARIANCES (FINAL CLUSTERS)\n",
    "# ============================================================\n",
    "cent_train, var_train = compute_centroids_and_variances(X_train, train_final)\n",
    "\n",
    "# ============================================================\n",
    "# TEST VARIANCES (FINAL CLUSTERS)\n",
    "# ============================================================\n",
    "test_umap_df = umap_test_df[umap_cols]\n",
    "if hasattr(kmeans_final, \"feature_names_in_\"):\n",
    "    test_umap_df = test_umap_df[expected_cols]\n",
    "\n",
    "X_test = test_umap_df.to_numpy().astype(kmeans_final.cluster_centers_.dtype)\n",
    "test_final = umap_test_df[\"Final_Cluster_UMAP\"].to_numpy()\n",
    "\n",
    "_, var_test = compute_centroids_and_variances(X_test, test_final)\n",
    "\n",
    "# ============================================================\n",
    "# COMPARISON: TRAIN vs TEST WITH % CHANGE\n",
    "# ============================================================\n",
    "print(\"\\nWithin-cluster variance comparison (Final UMAP clusters):\")\n",
    "print(f\"{'Cluster':<8} {'Train Var':<18} {'Test Var':<18} {'% Change':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for cid in sorted(var_train.keys()):\n",
    "    tv = var_train[cid]\n",
    "    sv = var_test.get(cid, np.nan)\n",
    "    \n",
    "    if np.isnan(sv):\n",
    "        print(f\"{cid:<8} {tv:<18.4f} {'No test pts':<18} {'N/A':<12}\")\n",
    "    else:\n",
    "        pct = ((sv - tv) / tv * 100) if tv != 0 else np.nan\n",
    "        tag = \"✓ Stable\" if (not np.isnan(pct) and abs(pct) < 25) else \"⚠ Check\"\n",
    "        print(f\"{cid:<8} {tv:<18.4f} {sv:<18.4f} {pct:>+8.2f}%  {tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac9da4-18ef-4792-9c29-e8a2b9693953",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "423f3598-a48a-4cef-8244-425f97144e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Metric 2] Cluster Separation & Decision Boundary Confidence\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Train Set - Cluster Assignment Confidence:\n",
      "Cluster  Avg Sep Ratio      % Ambiguous (<0.3)   Confused With  \n",
      "----------------------------------------------------------------------\n",
      "0        2.017                 1.7%  ✓ Clear      Cluster 1\n",
      "1        0.758                26.7%  ❌ Fuzzy      Cluster 2\n",
      "2        1.079                24.8%  ⚠ Moderate   Cluster 1\n",
      "3        32.324                0.4%  ✓ Clear      Cluster 4\n",
      "4        3.704                 2.8%  ✓ Clear      Cluster 2\n",
      "5        1.256                16.0%  ⚠ Moderate   Cluster 6\n",
      "6        0.357                49.4%  ❌ Fuzzy      Cluster 5\n",
      "\n",
      "Test Set - Cluster Assignment Confidence:\n",
      "Cluster  Avg Sep Ratio      % Ambiguous (<0.3)   Confused With  \n",
      "----------------------------------------------------------------------\n",
      "0        2.087                 0.0%  ✓ Clear      Cluster 1\n",
      "1        0.579                32.9%  ❌ Fuzzy      Cluster 2\n",
      "2        0.860                31.7%  ❌ Fuzzy      Cluster 1\n",
      "3        30.495                0.6%  ✓ Clear      Cluster 4\n",
      "4        3.358                 3.9%  ✓ Clear      Cluster 2\n",
      "5        1.050                19.4%  ⚠ Moderate   Cluster 6\n",
      "6        0.354                49.4%  ❌ Fuzzy      Cluster 5\n",
      "\n",
      "⚠ Most Ambiguous Assignments in Test Set (Separation Ratio < 0.2):\n",
      "Found 154 ambiguous assignments (14.7% of test set)\n",
      "\n",
      "Index    Assigned   Could be   Sep Ratio   \n",
      "--------------------------------------------------\n",
      "133      6          2          -0.510      \n",
      "322      6          2          -0.458      \n",
      "374      6          2          -0.437      \n",
      "520      6          2          -0.433      \n",
      "778      6          2          -0.358      \n",
      "587      6          2          -0.349      \n",
      "754      6          2          -0.340      \n",
      "463      6          2          -0.301      \n",
      "234      6          2          -0.287      \n",
      "1010     6          2          -0.251      \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Metric 2] Cluster Separation & Decision Boundary Confidence\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def calculate_separation_metrics(X, labels, centroids):\n",
    "    \"\"\"\n",
    "    For each point, calculate:\n",
    "    1. Distance to assigned cluster centroid (d1)\n",
    "    2. Distance to nearest other cluster centroid (d2)\n",
    "    3. Separation ratio: (d2 - d1) / d1 (higher = more confident assignment)\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'cluster': [],\n",
    "        'assigned_distance': [],\n",
    "        'nearest_other_distance': [],\n",
    "        'separation_ratio': [],\n",
    "        'nearest_other_cluster': []\n",
    "    }\n",
    "    \n",
    "    for i, point in enumerate(X):\n",
    "        assigned_cluster = labels[i]\n",
    "        \n",
    "        # Distance to assigned centroid\n",
    "        d1 = np.linalg.norm(point - centroids[assigned_cluster])\n",
    "        \n",
    "        # Find nearest other cluster\n",
    "        other_distances = {}\n",
    "        for cid, centroid in centroids.items():\n",
    "            if cid != assigned_cluster:\n",
    "                other_distances[cid] = np.linalg.norm(point - centroid)\n",
    "        \n",
    "        nearest_other_cluster = min(other_distances, key=other_distances.get)\n",
    "        d2 = other_distances[nearest_other_cluster]\n",
    "        \n",
    "        # Separation ratio: higher means more confident assignment\n",
    "        sep_ratio = (d2 - d1) / d1 if d1 > 0 else np.inf\n",
    "        \n",
    "        results['cluster'].append(assigned_cluster)\n",
    "        results['assigned_distance'].append(d1)\n",
    "        results['nearest_other_distance'].append(d2)\n",
    "        results['separation_ratio'].append(sep_ratio)\n",
    "        results['nearest_other_cluster'].append(nearest_other_cluster)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Calculate for train\n",
    "sep_train = calculate_separation_metrics(X_train, train_final, cent_train)\n",
    "\n",
    "# Calculate for test\n",
    "sep_test = calculate_separation_metrics(X_test, test_final, cent_train)\n",
    "\n",
    "# Summary by cluster\n",
    "print(\"\\nTrain Set - Cluster Assignment Confidence:\")\n",
    "print(f\"{'Cluster':<8} {'Avg Sep Ratio':<18} {'% Ambiguous (<0.3)':<20} {'Confused With':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cid in sorted(sep_train['cluster'].unique()):\n",
    "    cluster_data = sep_train[sep_train['cluster'] == cid]\n",
    "    avg_sep = cluster_data['separation_ratio'].mean()\n",
    "    pct_ambiguous = (cluster_data['separation_ratio'] < 0.3).mean() * 100\n",
    "    \n",
    "    # Most common confused cluster\n",
    "    confused_with = cluster_data['nearest_other_cluster'].mode()[0] if len(cluster_data) > 0 else 'N/A'\n",
    "    \n",
    "    status = \"✓ Clear\" if pct_ambiguous < 10 else (\"⚠ Moderate\" if pct_ambiguous < 25 else \"❌ Fuzzy\")\n",
    "    print(f\"{cid:<8} {avg_sep:<18.3f} {pct_ambiguous:>6.1f}%  {status:<12} Cluster {confused_with}\")\n",
    "\n",
    "print(\"\\nTest Set - Cluster Assignment Confidence:\")\n",
    "print(f\"{'Cluster':<8} {'Avg Sep Ratio':<18} {'% Ambiguous (<0.3)':<20} {'Confused With':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cid in sorted(sep_test['cluster'].unique()):\n",
    "    cluster_data = sep_test[sep_test['cluster'] == cid]\n",
    "    avg_sep = cluster_data['separation_ratio'].mean()\n",
    "    pct_ambiguous = (cluster_data['separation_ratio'] < 0.3).mean() * 100\n",
    "    \n",
    "    confused_with = cluster_data['nearest_other_cluster'].mode()[0] if len(cluster_data) > 0 else 'N/A'\n",
    "    \n",
    "    status = \"✓ Clear\" if pct_ambiguous < 10 else (\"⚠ Moderate\" if pct_ambiguous < 25 else \"❌ Fuzzy\")\n",
    "    print(f\"{cid:<8} {avg_sep:<18.3f} {pct_ambiguous:>6.1f}%  {status:<12} Cluster {confused_with}\")\n",
    "\n",
    "# Identify most ambiguous assignments in test set\n",
    "print(\"\\n⚠ Most Ambiguous Assignments in Test Set (Separation Ratio < 0.2):\")\n",
    "ambiguous = sep_test[sep_test['separation_ratio'] < 0.2].copy()\n",
    "ambiguous = ambiguous.sort_values('separation_ratio')\n",
    "print(f\"Found {len(ambiguous)} ambiguous assignments ({len(ambiguous)/len(sep_test)*100:.1f}% of test set)\")\n",
    "print(f\"\\n{'Index':<8} {'Assigned':<10} {'Could be':<10} {'Sep Ratio':<12}\")\n",
    "print(\"-\"*50)\n",
    "for idx, row in ambiguous.head(10).iterrows():\n",
    "    print(f\"{idx:<8} {int(row['cluster']):<10} {int(row['nearest_other_cluster']):<10} {row['separation_ratio']:<12.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "591cbb4f-cb91-4294-84a8-4fac7c8df079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Metric 3] Feature Contribution to Cluster Assignment\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Top 3 Distinguishing Features per Cluster (Train Set):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Cluster 0:\n",
      "  UMAP_7          Importance: 1.981\n",
      "  UMAP_5          Importance: 1.695\n",
      "  UMAP_10         Importance: 1.020\n",
      "\n",
      "Cluster 1:\n",
      "  UMAP_7          Importance: 2.090\n",
      "  UMAP_18         Importance: 1.151\n",
      "  UMAP_19         Importance: 1.136\n",
      "\n",
      "Cluster 2:\n",
      "  UMAP_9          Importance: 1.594\n",
      "  UMAP_10         Importance: 1.503\n",
      "  UMAP_7          Importance: 1.278\n",
      "\n",
      "Cluster 3:\n",
      "  UMAP_0          Importance: 53.883\n",
      "  UMAP_12         Importance: 16.411\n",
      "  UMAP_3          Importance: 14.678\n",
      "\n",
      "Cluster 4:\n",
      "  UMAP_17         Importance: 2.524\n",
      "  UMAP_11         Importance: 2.158\n",
      "  UMAP_10         Importance: 1.018\n",
      "\n",
      "Cluster 5:\n",
      "  UMAP_7          Importance: 1.657\n",
      "  UMAP_8          Importance: 1.657\n",
      "  UMAP_16         Importance: 1.254\n",
      "\n",
      "Cluster 6:\n",
      "  UMAP_15         Importance: 1.207\n",
      "  UMAP_1          Importance: 1.129\n",
      "  UMAP_18         Importance: 0.996\n",
      "\n",
      "\n",
      "Feature Importance Consistency (Train vs Test):\n",
      "Cluster  Train Top Feat       Test Top Feat        Match?    \n",
      "----------------------------------------------------------------------\n",
      "0        UMAP_7               UMAP_7               ✓         \n",
      "1        UMAP_7               UMAP_7               ✓         \n",
      "2        UMAP_9               UMAP_9               ✓         \n",
      "3        UMAP_0               UMAP_0               ✓         \n",
      "4        UMAP_17              UMAP_17              ✓         \n",
      "5        UMAP_7               UMAP_8               ✗         \n",
      "6        UMAP_15              UMAP_15              ✓         \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Metric 3] Feature Contribution to Cluster Assignment\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def calculate_feature_importance_for_assignment(X, labels, centroids, feature_names):\n",
    "    \"\"\"\n",
    "    For each cluster, calculate which features most distinguish it from others.\n",
    "    Uses the ratio of within-cluster variance to between-cluster variance per feature.\n",
    "    \"\"\"\n",
    "    feature_importance = {}\n",
    "    \n",
    "    for cid in np.unique(labels):\n",
    "        cluster_points = X[labels == cid]\n",
    "        other_points = X[labels != cid]\n",
    "        \n",
    "        if len(cluster_points) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For each feature, calculate separation\n",
    "        importance_scores = []\n",
    "        for f in range(X.shape[1]):\n",
    "            cluster_vals = cluster_points[:, f]\n",
    "            other_vals = other_points[:, f]\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            mean_diff = abs(cluster_vals.mean() - other_vals.mean())\n",
    "            pooled_std = np.sqrt((cluster_vals.std()**2 + other_vals.std()**2) / 2)\n",
    "            \n",
    "            if pooled_std > 0:\n",
    "                cohens_d = mean_diff / pooled_std\n",
    "            else:\n",
    "                cohens_d = 0\n",
    "            \n",
    "            importance_scores.append(cohens_d)\n",
    "        \n",
    "        feature_importance[cid] = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance_scores\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Get feature names (original features before UMAP)\n",
    "# You'll need to use your original feature columns here\n",
    "# For now, if you want to do this on UMAP dimensions:\n",
    "umap_feature_names = [f\"UMAP_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "feat_importance_train = calculate_feature_importance_for_assignment(\n",
    "    X_train, train_final, cent_train, umap_feature_names\n",
    ")\n",
    "\n",
    "print(\"\\nTop 3 Distinguishing Features per Cluster (Train Set):\")\n",
    "print(\"-\"*70)\n",
    "for cid in sorted(feat_importance_train.keys()):\n",
    "    print(f\"\\nCluster {cid}:\")\n",
    "    top_features = feat_importance_train[cid].head(3)\n",
    "    for _, row in top_features.iterrows():\n",
    "        print(f\"  {row['Feature']:<15} Importance: {row['Importance']:.3f}\")\n",
    "\n",
    "# For TEST set - check if feature importance pattern holds\n",
    "feat_importance_test = calculate_feature_importance_for_assignment(\n",
    "    X_test, test_final, cent_train, umap_feature_names\n",
    ")\n",
    "\n",
    "print(\"\\n\\nFeature Importance Consistency (Train vs Test):\")\n",
    "print(f\"{'Cluster':<8} {'Train Top Feat':<20} {'Test Top Feat':<20} {'Match?':<10}\")\n",
    "print(\"-\"*70)\n",
    "for cid in sorted(feat_importance_train.keys()):\n",
    "    if cid in feat_importance_test:\n",
    "        train_top = feat_importance_train[cid].iloc[0]['Feature']\n",
    "        test_top = feat_importance_test[cid].iloc[0]['Feature']\n",
    "        match = \"✓\" if train_top == test_top else \"✗\"\n",
    "        print(f\"{cid:<8} {train_top:<20} {test_top:<20} {match:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "635d45b9-1ee3-4c40-88a1-6914d8689ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Metric 4] Cluster Membership Stability\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Cluster Assignment Stability (Test Set):\n",
      "Cluster  Mean Stability     % Unstable (<0.7)    Status    \n",
      "----------------------------------------------------------------------\n",
      "0        1.000                 0.0%  ✓ Robust        ✓ Robust\n",
      "1        0.925                 9.0%  ✓ Robust        ✓ Robust\n",
      "2        0.920                11.7%  ⚠ Moderate      ⚠ Moderate\n",
      "3        1.000                 0.0%  ✓ Robust        ✓ Robust\n",
      "4        0.995                 0.0%  ✓ Robust        ✓ Robust\n",
      "5        0.972                 3.0%  ✓ Robust        ✓ Robust\n",
      "6        0.859                15.6%  ⚠ Moderate      ⚠ Moderate\n",
      "\n",
      "Overall Test Set Stability: 0.954\n",
      "Samples with stability < 0.7: 55 (5.2%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Metric 4] Cluster Membership Stability\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def calculate_stability_score(X, labels, centroids, n_bootstrap=50):\n",
    "    \"\"\"\n",
    "    For each point, see how often it gets assigned to the same cluster\n",
    "    when we add noise or bootstrap sample the data.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    stability_scores = np.zeros(n_samples)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Add small Gaussian noise\n",
    "        noise_std = 0.05 * X.std(axis=0)  # 5% noise\n",
    "        X_noisy = X + np.random.normal(0, noise_std, X.shape)\n",
    "        \n",
    "        # Reassign to nearest centroid\n",
    "        new_labels = np.array([\n",
    "            min(centroids.keys(), \n",
    "                key=lambda c: np.linalg.norm(X_noisy[i] - centroids[c]))\n",
    "            for i in range(n_samples)\n",
    "        ])\n",
    "        \n",
    "        # Check if assignment matches original\n",
    "        stability_scores += (new_labels == labels).astype(int)\n",
    "    \n",
    "    return stability_scores / n_bootstrap\n",
    "\n",
    "# Calculate stability for test set\n",
    "stability_test = calculate_stability_score(X_test, test_final, cent_train, n_bootstrap=50)\n",
    "\n",
    "# Summary by cluster\n",
    "stability_df = pd.DataFrame({\n",
    "    'cluster': test_final,\n",
    "    'stability': stability_test\n",
    "})\n",
    "\n",
    "print(\"\\nCluster Assignment Stability (Test Set):\")\n",
    "print(f\"{'Cluster':<8} {'Mean Stability':<18} {'% Unstable (<0.7)':<20} {'Status':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for cid in sorted(stability_df['cluster'].unique()):\n",
    "    cluster_stab = stability_df[stability_df['cluster'] == cid]['stability']\n",
    "    mean_stab = cluster_stab.mean()\n",
    "    pct_unstable = (cluster_stab < 0.7).mean() * 100\n",
    "    \n",
    "    status = \"✓ Robust\" if pct_unstable < 10 else (\"⚠ Moderate\" if pct_unstable < 25 else \"❌ Fragile\")\n",
    "    print(f\"{cid:<8} {mean_stab:<18.3f} {pct_unstable:>6.1f}%  {status:<15} {status}\")\n",
    "\n",
    "print(f\"\\nOverall Test Set Stability: {stability_test.mean():.3f}\")\n",
    "print(f\"Samples with stability < 0.7: {(stability_test < 0.7).sum()} ({(stability_test < 0.7).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f676c0ea-ca5a-4a3f-a304-ffd69dbfa5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_patient_cluster(\n",
    "    patient_features,          # 1D numpy array of UMAP features\n",
    "    centroids,                 # dict {cluster_id: centroid array}\n",
    "    feature_names,             # list of feature names (UMAP1, UMAP2, ...)\n",
    "    feature_importance_dict,   # dict {cluster_id: pd.DataFrame with Feature, Importance}\n",
    "    stability_score=None       # optional stability score for this patient\n",
    "):\n",
    "    # 1️⃣ Compute distances to all centroids\n",
    "    distances = {cid: np.linalg.norm(patient_features - c) for cid, c in centroids.items()}\n",
    "    \n",
    "    # Assigned cluster: nearest centroid\n",
    "    assigned_cluster = min(distances, key=distances.get)\n",
    "    distance_to_assigned = distances[assigned_cluster]\n",
    "    \n",
    "    # Nearest competing cluster\n",
    "    other_distances = {cid: d for cid, d in distances.items() if cid != assigned_cluster}\n",
    "    nearest_other_cluster = min(other_distances, key=other_distances.get)\n",
    "    distance_to_other = other_distances[nearest_other_cluster]\n",
    "    \n",
    "    # Separation ratio\n",
    "    separation_ratio = (distance_to_other - distance_to_assigned) / distance_to_assigned if distance_to_assigned > 0 else np.inf\n",
    "    \n",
    "    # 2️⃣ Top features pulling patient toward assigned cluster\n",
    "    # Use feature_importance_dict from Metric 3 (cohen's d per feature)\n",
    "    importance_df = feature_importance_dict.get(assigned_cluster, pd.DataFrame())\n",
    "    if not importance_df.empty:\n",
    "        # Match features with patient differences from centroid\n",
    "        contributions = {}\n",
    "        for _, row in importance_df.iterrows():\n",
    "            feat = row['Feature']\n",
    "            # Absolute difference from assigned cluster centroid\n",
    "            feat_idx = feature_names.index(feat)\n",
    "            contributions[feat] = abs(patient_features[feat_idx] - centroids[assigned_cluster][feat_idx]) * row['Importance']\n",
    "        # Top 3 features\n",
    "        top_features = sorted(contributions.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        top_features_list = [{'Feature': f, 'Contribution': round(c, 3)} for f, c in top_features]\n",
    "    else:\n",
    "        top_features_list = []\n",
    "\n",
    "    # 3️⃣ Construct summary\n",
    "    summary_text = (\n",
    "        f\"Patient is assigned to Cluster {assigned_cluster} because they are \"\n",
    "        f\"{separation_ratio:.2f} separation ratio closer in UMAP space compared to Cluster {nearest_other_cluster}. \"\n",
    "    )\n",
    "    if top_features_list:\n",
    "        feat_names = ', '.join([f['Feature'] for f in top_features_list])\n",
    "        summary_text += f\"Key features driving this assignment include {feat_names}. \"\n",
    "    if stability_score is not None:\n",
    "        summary_text += f\"Cluster membership stability = {stability_score:.2f}.\"\n",
    "    \n",
    "    # 4️⃣ Return structured explanation\n",
    "    return {\n",
    "        'assigned_cluster': assigned_cluster,\n",
    "        'nearest_other_cluster': nearest_other_cluster,\n",
    "        'distance_to_assigned': distance_to_assigned,\n",
    "        'distance_to_other': distance_to_other,\n",
    "        'separation_ratio': separation_ratio,\n",
    "        'top_features_pulling_to_assigned': top_features_list,\n",
    "        'stability_score': stability_score,\n",
    "        'summary': summary_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "924f3e9b-3005-4e2f-83f0-424a034a3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_umap_feature_map(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    umap_map = {}\n",
    "\n",
    "    # For UMAP1 to UMAP20\n",
    "    for i in range(1, 21):\n",
    "        feature_col = f\"UMAP{i}_Feature\"\n",
    "        corr_col = f\"UMAP{i}_Correlation\"\n",
    "\n",
    "        if feature_col in df.columns and corr_col in df.columns:\n",
    "            # Always take the FIRST row = highest-correlation feature\n",
    "            best_feature = df.loc[0, feature_col]\n",
    "            best_corr = df.loc[0, corr_col]\n",
    "\n",
    "            # Store in normalized form \"UMAP_i\"\n",
    "            umap_map[f\"UMAP_{i}\"] = {\n",
    "                \"feature\": best_feature,\n",
    "                \"correlation\": float(best_corr)\n",
    "            }\n",
    "\n",
    "    return umap_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e74d0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_umap_to_original(result, umap_map):\n",
    "    translated = []\n",
    "\n",
    "    for item in result[\"top_features_pulling_to_assigned\"]:\n",
    "        umap_feature = item[\"Feature\"]  # like \"UMAP_8\"\n",
    "        contrib = item[\"Contribution\"]\n",
    "\n",
    "        if umap_feature in umap_map:\n",
    "            translated.append({\n",
    "                \"Original_Feature\": umap_map[umap_feature][\"feature\"],\n",
    "                \"Correlation\": umap_map[umap_feature][\"correlation\"],\n",
    "                \"Contribution\": contrib\n",
    "            })\n",
    "        else:\n",
    "            print(f\"[Warning] No mapped feature for {umap_feature}\")\n",
    "\n",
    "    # sort by contribution descending\n",
    "    translated = sorted(translated, key=lambda x: x[\"Contribution\"], reverse=True)\n",
    "    return translated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1817a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_summary(result, original_features):\n",
    "    if original_features:\n",
    "        top_feats = \", \".join([f[\"Original_Feature\"] for f in original_features[:3]])\n",
    "    else:\n",
    "        top_feats = \"N/A\"\n",
    "\n",
    "    result[\"summary\"] = (\n",
    "        f\"Patient is assigned to Cluster {result['assigned_cluster']} because they are \"\n",
    "        f\"{result['separation_ratio']:.2f} separation ratio closer in UMAP space compared to Cluster \"\n",
    "        f\"{result['nearest_other_cluster']}. Key original health features driving this assignment include \"\n",
    "        f\"{top_feats}. Cluster membership stability = {result['stability_score']:.2f}.\"\n",
    "    )\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58f60c",
   "metadata": {},
   "source": [
    "## Test Patient 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "06761448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assigned_cluster': 6,\n",
      " 'distance_to_assigned': 0.7190856120113591,\n",
      " 'distance_to_other': 1.1497893264882875,\n",
      " 'nearest_other_cluster': 5,\n",
      " 'separation_ratio': 0.5989602729947608,\n",
      " 'stability_score': 1.0,\n",
      " 'summary': 'Patient is assigned to Cluster 6 because they are 0.60 separation '\n",
      "            'ratio closer in UMAP space compared to Cluster 5. Key features '\n",
      "            'driving this assignment include UMAP_1, UMAP_13, UMAP_8. Cluster '\n",
      "            'membership stability = 1.00.',\n",
      " 'top_features_pulling_to_assigned': [{'Contribution': 0.322,\n",
      "                                       'Feature': 'UMAP_1'},\n",
      "                                      {'Contribution': 0.256,\n",
      "                                       'Feature': 'UMAP_13'},\n",
      "                                      {'Contribution': 0.217,\n",
      "                                       'Feature': 'UMAP_8'}]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage for first patient in test set\n",
    "patient_idx = 0\n",
    "patient_features = X_test[patient_idx]\n",
    "stability = stability_test[patient_idx]\n",
    "explanation = explain_patient_cluster(\n",
    "    patient_features,\n",
    "    cent_train,\n",
    "    umap_feature_names,\n",
    "    feat_importance_train,\n",
    "    stability_score=stability\n",
    ")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5ce6dcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assigned_cluster': 6,\n",
       " 'nearest_other_cluster': 5,\n",
       " 'distance_to_assigned': 0.7190856120113591,\n",
       " 'distance_to_other': 1.1497893264882875,\n",
       " 'separation_ratio': 0.5989602729947608,\n",
       " 'top_features_pulling_to_assigned': [{'Feature': 'UMAP_1',\n",
       "   'Contribution': 0.322},\n",
       "  {'Feature': 'UMAP_13', 'Contribution': 0.256},\n",
       "  {'Feature': 'UMAP_8', 'Contribution': 0.217}],\n",
       " 'stability_score': 1.0,\n",
       " 'summary': 'Patient is assigned to Cluster 6 because they are 0.60 separation ratio closer in UMAP space compared to Cluster 5. Key original health features driving this assignment include Has_Kidney_Failure, Has_Kidney_Failure, mean_steroid_ng_dl. Cluster membership stability = 1.00.',\n",
       " 'top_original_features': [{'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.964724305029977,\n",
       "   'Contribution': 0.322},\n",
       "  {'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.959479492745399,\n",
       "   'Contribution': 0.256},\n",
       "  {'Original_Feature': 'mean_steroid_ng_dl',\n",
       "   'Correlation': 0.4647739244486011,\n",
       "   'Contribution': 0.217}]}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_map = load_umap_feature_map(\"umap_health_features_with_correlations.csv\")\n",
    "\n",
    "orig_features = translate_umap_to_original(explanation, umap_map)\n",
    "\n",
    "explanation[\"top_original_features\"] = orig_features\n",
    "\n",
    "explanation = update_summary(explanation, orig_features)\n",
    "\n",
    "explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784bf7e",
   "metadata": {},
   "source": [
    "## Test Patient 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "32660818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assigned_cluster': 6,\n",
      " 'distance_to_assigned': 0.9741596330208133,\n",
      " 'distance_to_other': 1.3371001081132543,\n",
      " 'nearest_other_cluster': 2,\n",
      " 'separation_ratio': 0.3725677628080147,\n",
      " 'stability_score': 1.0,\n",
      " 'summary': 'Patient is assigned to Cluster 6 because they are 0.37 separation '\n",
      "            'ratio closer in UMAP space compared to Cluster 2. Key features '\n",
      "            'driving this assignment include UMAP_18, UMAP_13, UMAP_19. '\n",
      "            'Cluster membership stability = 1.00.',\n",
      " 'top_features_pulling_to_assigned': [{'Contribution': 0.33,\n",
      "                                       'Feature': 'UMAP_18'},\n",
      "                                      {'Contribution': 0.317,\n",
      "                                       'Feature': 'UMAP_13'},\n",
      "                                      {'Contribution': 0.265,\n",
      "                                       'Feature': 'UMAP_19'}]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage for first patient in test set\n",
    "patient_idx = 10\n",
    "patient_features = X_test[patient_idx]\n",
    "stability = stability_test[patient_idx]\n",
    "explanation = explain_patient_cluster(\n",
    "    patient_features,\n",
    "    cent_train,\n",
    "    umap_feature_names,\n",
    "    feat_importance_train,\n",
    "    stability_score=stability\n",
    ")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ac93f6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assigned_cluster': 6,\n",
       " 'nearest_other_cluster': 2,\n",
       " 'distance_to_assigned': 0.9741596330208133,\n",
       " 'distance_to_other': 1.3371001081132543,\n",
       " 'separation_ratio': 0.3725677628080147,\n",
       " 'top_features_pulling_to_assigned': [{'Feature': 'UMAP_18',\n",
       "   'Contribution': 0.33},\n",
       "  {'Feature': 'UMAP_13', 'Contribution': 0.317},\n",
       "  {'Feature': 'UMAP_19', 'Contribution': 0.265}],\n",
       " 'stability_score': 1.0,\n",
       " 'summary': 'Patient is assigned to Cluster 6 because they are 0.37 separation ratio closer in UMAP space compared to Cluster 2. Key original health features driving this assignment include Has_Kidney_Failure, Has_Kidney_Failure, Has_Kidney_Failure. Cluster membership stability = 1.00.',\n",
       " 'top_original_features': [{'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.7471517552457481,\n",
       "   'Contribution': 0.33},\n",
       "  {'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.959479492745399,\n",
       "   'Contribution': 0.317},\n",
       "  {'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.9179585731525768,\n",
       "   'Contribution': 0.265}]}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_map = load_umap_feature_map(\"umap_health_features_with_correlations.csv\")\n",
    "\n",
    "orig_features = translate_umap_to_original(explanation, umap_map)\n",
    "\n",
    "explanation[\"top_original_features\"] = orig_features\n",
    "\n",
    "explanation = update_summary(explanation, orig_features)\n",
    "\n",
    "explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb815e9",
   "metadata": {},
   "source": [
    "## Test Patient 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "96e7d03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assigned_cluster': 4,\n",
      " 'distance_to_assigned': 0.8744675705825135,\n",
      " 'distance_to_other': 3.1589402874366552,\n",
      " 'nearest_other_cluster': 2,\n",
      " 'separation_ratio': 2.6124155928759882,\n",
      " 'stability_score': 1.0,\n",
      " 'summary': 'Patient is assigned to Cluster 4 because they are 2.61 separation '\n",
      "            'ratio closer in UMAP space compared to Cluster 2. Key features '\n",
      "            'driving this assignment include UMAP_17, UMAP_8, UMAP_11. Cluster '\n",
      "            'membership stability = 1.00.',\n",
      " 'top_features_pulling_to_assigned': [{'Contribution': 0.496,\n",
      "                                       'Feature': 'UMAP_17'},\n",
      "                                      {'Contribution': 0.475,\n",
      "                                       'Feature': 'UMAP_8'},\n",
      "                                      {'Contribution': 0.243,\n",
      "                                       'Feature': 'UMAP_11'}]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage for first patient in test set\n",
    "patient_idx = 47\n",
    "patient_features = X_test[patient_idx]\n",
    "stability = stability_test[patient_idx]\n",
    "explanation = explain_patient_cluster(\n",
    "    patient_features,\n",
    "    cent_train,\n",
    "    umap_feature_names,\n",
    "    feat_importance_train,\n",
    "    stability_score=stability\n",
    ")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "df6bbc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assigned_cluster': 4,\n",
       " 'nearest_other_cluster': 2,\n",
       " 'distance_to_assigned': 0.8744675705825135,\n",
       " 'distance_to_other': 3.1589402874366552,\n",
       " 'separation_ratio': 2.6124155928759882,\n",
       " 'top_features_pulling_to_assigned': [{'Feature': 'UMAP_17',\n",
       "   'Contribution': 0.496},\n",
       "  {'Feature': 'UMAP_8', 'Contribution': 0.475},\n",
       "  {'Feature': 'UMAP_11', 'Contribution': 0.243}],\n",
       " 'stability_score': 1.0,\n",
       " 'summary': 'Patient is assigned to Cluster 4 because they are 2.61 separation ratio closer in UMAP space compared to Cluster 2. Key original health features driving this assignment include Has_Kidney_Failure, mean_steroid_ng_dl, blood_macros. Cluster membership stability = 1.00.',\n",
       " 'top_original_features': [{'Original_Feature': 'Has_Kidney_Failure',\n",
       "   'Correlation': 0.9412905841920932,\n",
       "   'Contribution': 0.496},\n",
       "  {'Original_Feature': 'mean_steroid_ng_dl',\n",
       "   'Correlation': 0.4647739244486011,\n",
       "   'Contribution': 0.475},\n",
       "  {'Original_Feature': 'blood_macros',\n",
       "   'Correlation': 0.323578178194059,\n",
       "   'Contribution': 0.243}]}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_map = load_umap_feature_map(\"umap_health_features_with_correlations.csv\")\n",
    "\n",
    "orig_features = translate_umap_to_original(explanation, umap_map)\n",
    "\n",
    "explanation[\"top_original_features\"] = orig_features\n",
    "\n",
    "explanation = update_summary(explanation, orig_features)\n",
    "\n",
    "explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b7086-ea26-4554-a8eb-a94d85280aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
