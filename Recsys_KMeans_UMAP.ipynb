{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753bbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def preprocess_new_record(new_record, pipeline_path='Data/preprocessing_pipeline.pkl'):\n",
    "    \"\"\"\n",
    "    Apply the same preprocessing to new records\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    new_record : dict or pd.DataFrame\n",
    "        New record(s) to preprocess\n",
    "    pipeline_path : str\n",
    "        Path to saved preprocessing pipeline\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Preprocessed record matching training data format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pipeline\n",
    "    with open(pipeline_path, 'rb') as f:\n",
    "        pipeline = pickle.load(f)\n",
    "    \n",
    "    # Convert to DataFrame if needed\n",
    "    if isinstance(new_record, dict):\n",
    "        df_new = pd.DataFrame([new_record])\n",
    "    else:\n",
    "        df_new = new_record.copy()\n",
    "    \n",
    "    # Step 1: Normalize Unknown-like responses\n",
    "    df_new = df_new.replace({\n",
    "        \"Don't know\": \"Unknown\", \"Refused\": \"Unknown\", \n",
    "        \"Not Applicable\": \"Unknown\", \"N/A\": \"Unknown\", \n",
    "        \"Unknown/NA\": \"Unknown\"\n",
    "    })\n",
    "    \n",
    "    # Step 2: Binary encoding\n",
    "    for col in pipeline['binary_cols']:\n",
    "        if col not in df_new.columns:\n",
    "            continue\n",
    "        \n",
    "        if col == \"Has_diabetes\":\n",
    "            mapping = pipeline['binary_mappings'][\"Has_diabetes\"]\n",
    "        elif col == \"Received_Hepatitis_A_Vaccine\":\n",
    "            mapping = pipeline['binary_mappings'][\"Received_Hepatitis_A_Vaccine\"]\n",
    "        else:\n",
    "            mapping = pipeline['binary_mappings'][\"default\"]\n",
    "        \n",
    "        df_new[col] = df_new[col].map(mapping)\n",
    "    \n",
    "    # Step 3: Ordinal encoding\n",
    "    for col, encoder in pipeline['ordinal_encoders'].items():\n",
    "        if col in df_new.columns:\n",
    "            df_new[col] = encoder.transform(df_new[[col]])\n",
    "    \n",
    "    # Step 4: One-hot encoding\n",
    "    for base_col in pipeline['ohe_cols']:\n",
    "        if base_col in df_new.columns:\n",
    "            # Get dummies for this column\n",
    "            dummies = pd.get_dummies(df_new[base_col], prefix=base_col, dtype=int)\n",
    "            \n",
    "            # Add any missing columns from training\n",
    "            for train_col in pipeline['ohe_column_names']:\n",
    "                if base_col in train_col and train_col not in dummies.columns:\n",
    "                    dummies[train_col] = 0\n",
    "            \n",
    "            # Remove extra columns not in training\n",
    "            cols_to_keep = [col for col in dummies.columns \n",
    "                           if col in pipeline['ohe_column_names']]\n",
    "            dummies = dummies[cols_to_keep]\n",
    "            \n",
    "            # Add to dataframe\n",
    "            df_new = pd.concat([df_new.drop(columns=[base_col]), dummies], axis=1)\n",
    "    \n",
    "    # Step 5: Apply log transformation to skewed columns\n",
    "    for col in pipeline['skewed_cols']:\n",
    "        if col in df_new.columns:\n",
    "            df_new[col] = np.log1p(df_new[col].clip(lower=0))\n",
    "    \n",
    "    # Step 6: Ensure all columns from training exist\n",
    "    for col in pipeline['all_columns']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 0  # Add missing columns with default value\n",
    "    \n",
    "    # Step 7: Reorder columns to match training data\n",
    "    df_new = df_new[pipeline['all_columns']]\n",
    "    \n",
    "    # Step 8: Apply scaling\n",
    "    df_new[pipeline['cols_to_scale']] = pipeline['scaler'].transform(\n",
    "        df_new[pipeline['cols_to_scale']]\n",
    "    )\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c1d87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of processed record: (1050, 55)\n",
      "Matches training data shape: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RobustScaler from version 1.5.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator OrdinalEncoder from version 1.5.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but OrdinalEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but OrdinalEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but OrdinalEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but OrdinalEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but OrdinalEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but OrdinalEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example new patient record\n",
    "test_df = pd.read_csv('Data/test_dataset.csv')\n",
    "df_scaled = pd.read_csv('Data/df_scaled.csv')\n",
    "df_scaled.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "# Preprocess the new record\n",
    "processed_patient = preprocess_new_record(test_df)\n",
    "\n",
    "print(f\"Shape of processed record: {processed_patient.shape}\")\n",
    "print(f\"Matches training data shape: {processed_patient.shape[1] == df_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee0418",
   "metadata": {},
   "source": [
    "# Dim Reduction UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f78dd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 15)\n",
      "        PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
      "0  1.113172 -1.533393  0.758375 -3.098632  0.347120 -2.097073 -0.264846   \n",
      "1 -5.575248  2.321419 -0.286932  0.385995  0.209788  0.111568 -0.681061   \n",
      "2  1.076240 -1.809363 -1.359945  1.711623  2.563675 -0.929339  3.114684   \n",
      "3  3.553449  3.006607  0.907166  1.245952 -1.023796  1.045440 -0.585587   \n",
      "4 -5.434803  2.295273  0.305454  1.150900  0.947202 -0.680264  0.273486   \n",
      "\n",
      "        PC8       PC9      PC10      PC11      PC12      PC13      PC14  \\\n",
      "0 -1.656722  0.981297 -1.057477  0.448272  0.554567 -0.070196  0.073864   \n",
      "1 -0.206276  0.706200 -0.047509 -0.798700  0.577931  0.197507  0.301032   \n",
      "2  1.257197  0.498221 -1.351363 -0.206696 -2.048451  0.044042 -0.820650   \n",
      "3  0.675189  1.765725  0.710279  0.598552  0.093380  0.288720  0.813715   \n",
      "4  0.302993  0.038955 -1.227088 -1.357034  1.056881 -0.174455  0.533989   \n",
      "\n",
      "       PC15  \n",
      "0 -0.988990  \n",
      "1  0.512794  \n",
      "2 -1.680909  \n",
      "3  0.179806  \n",
      "4 -0.408397  \n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "umap = joblib.load('Data/umap_model.pkl')\n",
    "umap_test = umap.transform(processed_patient)\n",
    "umap_test_df = pd.DataFrame(umap_test, columns=[f'PC{i+1}' for i in range(umap_test.shape[1])])\n",
    "umap_test_df.head()\n",
    "\n",
    "print(umap_test_df.shape)\n",
    "print(umap_test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5b1d0",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a75596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING CLUSTERING MODELS\n",
      "======================================================================\n",
      "✓ All models loaded successfully\n",
      "\n",
      "======================================================================\n",
      "ASSIGNING CLUSTERS TO TEST PATIENTS\n",
      "======================================================================\n",
      "Test data shape: (1050, 15)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- PC1\n- PC10\n- PC11\n- PC12\n- PC13\n- ...\nFeature names seen at fit time, yet now missing:\n- UMAP1\n- UMAP10\n- UMAP11\n- UMAP12\n- UMAP13\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mumap_test_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Step 1: Get initial cluster assignments (4 clusters)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m initial_clusters \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_test_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Step 2: Apply hierarchical logic to get FINAL clusters (0..6)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m final_clusters \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1085\u001b[0m, in \u001b[0;36m_BaseKMeans.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the closest cluster each sample in X belongs to.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03mIn the vector quantization literature, `cluster_centers_` is called\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;124;03m    Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1085\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_test_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;66;03m# sample weights are not used by predict but cython helpers expect an array\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:944\u001b[0m, in \u001b[0;36m_BaseKMeans._check_test_data\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_test_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m--> 944\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[1;32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2929\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[0;32m   2846\u001b[0m     _estimator,\n\u001b[0;32m   2847\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2853\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m   2854\u001b[0m ):\n\u001b[0;32m   2855\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[0;32m   2856\u001b[0m \n\u001b[0;32m   2857\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2927\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2929\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2930\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[0;32m   2931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[1;32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2787\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m   2785\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2787\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- PC1\n- PC10\n- PC11\n- PC12\n- PC13\n- ...\nFeature names seen at fit time, yet now missing:\n- UMAP1\n- UMAP10\n- UMAP11\n- UMAP12\n- UMAP13\n- ...\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# LOAD CLUSTERING MODELS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING CLUSTERING MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load clustering models\n",
    "kmeans_final = joblib.load('Models/kmeans_umap_initial4_model.pkl')\n",
    "kmeans_c0 = joblib.load('Models/kmeans_umap_c0.pkl')  \n",
    "kmeans_c3 = joblib.load('Models/kmeans_umap_c3.pkl')\n",
    "\n",
    "k_split_c0 = 3\n",
    "k_split_c3 = 2\n",
    "\n",
    "print(\"✓ All models loaded successfully\")\n",
    "\n",
    "# ============================================================\n",
    "# ASSIGN CLUSTERS TO ALL TEST PATIENTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ASSIGNING CLUSTERS TO TEST PATIENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Test data shape: {umap_test_df.shape}\")\n",
    "\n",
    "# Step 1: Get initial cluster assignments (4 clusters)\n",
    "initial_clusters = kmeans_final.predict(umap_test_df)\n",
    "\n",
    "# Step 2: Apply hierarchical logic to get FINAL clusters (0..6)\n",
    "final_clusters = []\n",
    "subcluster_assignments = []\n",
    "\n",
    "for idx in range(len(umap_test_df)):\n",
    "    main_cluster = initial_clusters[idx]\n",
    "    patient_features = umap_test_df.iloc[[idx]]   # keep as DataFrame for .predict\n",
    "    \n",
    "    if main_cluster == 0:\n",
    "        # Initial Cluster 0 splits into subclusters → Final clusters 0, 1, 2\n",
    "        subcluster_id = kmeans_c0.predict(patient_features)[0]  # {0,1,2}\n",
    "        final_cluster = 0 + subcluster_id                      # 0,1,2\n",
    "        final_clusters.append(int(final_cluster))\n",
    "        subcluster_assignments.append(int(subcluster_id))\n",
    "        \n",
    "    elif main_cluster == 1:\n",
    "        # Initial Cluster 1 stays as final cluster 3\n",
    "        final_clusters.append(3)\n",
    "        subcluster_assignments.append(None)\n",
    "        \n",
    "    elif main_cluster == 2:\n",
    "        # Initial Cluster 2 stays as final cluster 4\n",
    "        final_clusters.append(4)\n",
    "        subcluster_assignments.append(None)\n",
    "        \n",
    "    elif main_cluster == 3:\n",
    "        # Initial Cluster 3 splits into subclusters → Final clusters 5, 6\n",
    "        subcluster_id = kmeans_c3.predict(patient_features)[0] \n",
    "        final_cluster = 5 + subcluster_id                      #\n",
    "        final_clusters.append(int(final_cluster))\n",
    "        subcluster_assignments.append(int(subcluster_id))\n",
    "\n",
    "# Attach results to the test UMAP DataFrame\n",
    "umap_test_df_assigned = umap_test_df.copy()\n",
    "umap_test_df_assigned['Initial_Cluster_UMAP'] = initial_clusters\n",
    "umap_test_df_assigned['Subcluster_UMAP']      = subcluster_assignments\n",
    "umap_test_df_assigned['Final_Cluster_UMAP']   = final_clusters\n",
    "\n",
    "# ============================================================\n",
    "# DISPLAY RESULTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UMAP CLUSTER ASSIGNMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nInitial cluster distribution (4 clusters):\")\n",
    "print(umap_test_df_assigned['Initial_Cluster_UMAP'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nFinal cluster distribution (7 clusters):\")\n",
    "print(umap_test_df_assigned['Final_Cluster_UMAP'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nCluster mapping (UMAP-based):\")\n",
    "print(\"  Final Cluster 0: Initial Cluster 0 → Subcluster 0\")\n",
    "print(\"  Final Cluster 1: Initial Cluster 0 → Subcluster 1\")\n",
    "print(\"  Final Cluster 2: Initial Cluster 0 → Subcluster 2\")\n",
    "print(\"  Final Cluster 3: Initial Cluster 1 (no subclustering)\")\n",
    "print(\"  Final Cluster 4: Initial Cluster 2 (no subclustering)\")\n",
    "print(\"  Final Cluster 5: Initial Cluster 3 → Subcluster 0\")\n",
    "print(\"  Final Cluster 6: Initial Cluster 3 → Subcluster 1\")\n",
    "\n",
    "print(\"\\nFirst 20 patient assignments:\")\n",
    "print(umap_test_df_assigned[['Initial_Cluster_UMAP', 'Subcluster_UMAP', 'Final_Cluster_UMAP']].head(20))\n",
    "\n",
    "# Save results\n",
    "umap_test_df_assigned.to_csv('Data/test_patients_with_clusters_kmeans_umap.csv', index=False)\n",
    "print(\"\\n✓ Results saved to 'Data/test_patients_with_clusters_kmeans_umap.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8fd7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
